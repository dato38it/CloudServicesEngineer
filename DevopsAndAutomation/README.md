# Devops и автоматизация в Yandex Cloud
<p><strong>Task:</strong><br />Devops и автоматизация в Yandex Cloud</p>
<p><strong>Task:</strong><br />Начало работы в CLI. На этой практической работе мы установим утилиту yc, познакомимся с режимом подсказок --help и выполним несколько простых команд.<br /><strong>Decision:</strong><br />Первым делом скачайте и установите CLI (если вы ещё не сделали этого в предыдущих курсах).<br />Теперь настройте программу для работы с вашим аккаунтом и облаком. Для этого запустите командную строку и введите команду:<br />yc init <br />В консоли появится предложение перейти по ссылке, чтобы программа получила доступ к аккаунту и облаку. Перейдите по ссылке, согласитесь с условиями, затем скопируйте токен и вставьте его в окно командной строки.<br />Срок жизни токена &mdash; один год. Через год необходимо получить новый токен и повторить аутентификацию.<br />Если кто-то узнал ваш токен, отзовите его и запросите новый.<br />Продолжайте установку: выберите облако, каталог и зону доступности по умолчанию. Следуйте указаниям системы и завершите настройку.<br />При установке автоматически создается профиль default, в него записываются выбранные настройки.<br />Список профилей можно посмотреть командой:<br />yc config profile list <br />Флаг --help<br />На прошлом уроке вы узнали о флагах, которые используются при запуске команд. Пожалуй, самый важный и частый флаг &mdash; --help (сокращенно -h), поэтому поговорим о нём подробнее.<br />Команд для управления ресурсами облака много. Вы запомните некоторые, но помнить всё невозможно. Пользуйтесь флагом --help, когда пишете команду. В этом случае она не выполняется, а в консоль выводится информация о ресурсах, которыми управляет команда, и параметрах запуска.<br />Помните, на предыдущем уроке мы говорили о типичной структуре большинства команд CLI (сервис &mdash; ресурс &mdash; действие &mdash; флаги)?<br />Важная особенность флага --help заключается в том, что его можно применять для каждого уровня этой структуры и писать команду постепенно.<br />В этой практической работе нам понадобится ВМ в облаке. Если у вас нет ВМ &mdash; создайте её в консоли управления, как мы это делали в предыдущих курсах.<br />Допустим, вы хотите перезапустить эту ВМ, но не помните полный синтаксис команды.<br />Сначала вызовите описание сервиса Yandex Compute Cloud:<br />yc compute --help <br />Вы увидите синтаксис команды (раздел Usage), список ресурсов, которыми можно управлять (Groups), а также действий (Commands) и флагов<br />Вам нужна ВМ &mdash; ресурс instance. Теперь узнайте, как получить список активных ВМ и какая команда отвечает за перезапуск:<br />yc compute instance --help <br />Сначала получите список ВМ, это команда list:<br />yc compute instance list <br />Результатом выполнения команды будет список машин в том каталоге, где вы работаете, с именами и идентификаторами:<br />+----------------------+-------------+---------------+---------+----------------+-------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; ZONE ID&nbsp;&nbsp;&nbsp; | STATUS&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |<br />+----------------------+-------------+---------------+---------+----------------+-------------+<br />| fhm2p20bifmg3k3voda7 | my-instance | ru-central1-a | RUNNING | ХХХ.ХХХ.ХХХ.ХХ | ХХ.ХХХ.Х.ХХ |<br />+----------------------+-------------+---------------+---------+----------------+-------------+ <br />На шаге 2 вы нашли команду перезапуска &mdash; restart, теперь можно посмотреть её синтаксис:<br />yc compute instance restart --help <br />Наконец, вы получили полный синтаксис команды перезапуска. Примените её к ВМ:<br />yc compute instance restart --name &lt;имя_ВМ&gt; <br />По такому принципу вы можете сформировать в консоли любую команду.<br />Итак, подведём итоги. Чтобы уточнить синтаксис или порядок действий при работе с CLI, выберите один из трёх путей: найдите нужное действие на странице сервиса Yandex Cloud в документации и там переключитесь на вкладку CLI; найти команду в справочнике о yc; воспользуйтесь флагом --help, шаг за шагом уточняя возможности и синтаксис команды.<br />Синхронный и асинхронный режимы работы. Некоторые команды выполняются очень быстро. Например, создание каталога или просмотр настроек профиля. Такие команды можно выполнять подряд, одну за другой &mdash; без задержек.<br />Если при выполнении команды ресурс изменяет состояние, создается операция. Примеры операций &mdash; перезапуск ВМ после обновления или резервное копирование базы данных.<br />Если каждая следующая команда ждёт завершения предыдущей операции, такой режим выполнения называется синхронным. Когда какая-то операция в синхронном режиме выполняется долго, CLI выводит в консоли точки и время с начала операции, чтобы показать, что процесс не завис:<br />...1s...6s...12s...17s <br />Операция может выполняться довольно долго, а ожидание &mdash; затормозить процесс. В таких случаях важно оценить, нужен ли результат операции для выполнения следующих команд. Если нет &mdash; можно не ждать её завершения и сразу же переходить к следующей команде. Этот режим называется асинхронным.<br />Чтобы выполнить команду асинхронно, используйте флаг --async.<br />Перезапустите одну из ранее созданных ВМ в асинхронном режиме (в команде укажите имя этой ВМ):<br />yc compute instance restart --name &lt;имя_ВМ&gt; --async <br />В ответ на асинхронный вызов CLI выводит идентификатор операции (в поле id) и информацию о ней:<br />id: fhm5k7iq03rm2s7enhdk<br />description: Restart instance<br />created_at: "2021-03-27T08:32:47.562595036Z"<br />created_by: aje9cb7k03512mrugcee<br />modified_at: "2021-03-27T08:32:47.562595036Z"<br />metadata:<br />&nbsp; '@type': type.googleapis.com/yandex.cloud.compute.v1.RestartInstanceMetadata<br />&nbsp; instance_id: fhm2p20bifmg3k3voda7 <br />С помощью идентификатора операции вы можете проверить результаты её выполнения:<br />yc operation get &lt;идентификатор_операции&gt; <br />Когда операция завершится, вы увидите результат:<br />id: fhm5k7iq03rm2s7enhdk<br />...<br />done: true<br />... <br /><strong>Decision:</strong><br />$ yc init<br />$ yc config profile list<br />$ yc compute --help<br />$ yc compute instance --help<br />$ yc compute instance list<br />$ yc compute instance restart --help<br />$ yc compute instance restart --name ubuntu-test<br />$ yc compute instance restart --name ubuntu-test --async<br />$ yc operation get fhmq5a4aren4lmigbt03<br /><strong>Task:</strong><br />Создание виртуальных машин с помощью CLI. Создание ВМ &mdash; одна из самых сложных команд CLI, потому что в ней очень много параметров. <br />Давайте потренируемся работать с ней.&nbsp; Но сначала подготовим окружение.<br />Мы будем работать в каталоге по умолчанию. Создадим в нём сеть, в ней &mdash; три подсети, а затем по ВМ в каждой подсети.<br /><strong>Decision:</strong><br />Создайте сеть my-network в Virtual Private Cloud. Эта команда относится к группе vpc. <br />yc vpc network create --name my-network<br />Выполнение операции займёт какое-то время. В итоге сеть появится в каталоге, который вы выбрали в предыдущей практической работе.<br />Теперь создадим три подсети в разных зонах доступности (ru-central1-a, ru-central1-b, ru-central1-c). Чтобы проще было выполнять задания дальше, назовите их my-subnet-1, my-subnet-2 и my-subnet-3. А пространства IP-адресов для подсетей укажите, соответственно, как 192.168.1.0/24, 192.168.2.0/24 и 192.168.3.0/24.<br />Не забудьте указать, что вы создаёте подсети в новой сети, созданной на предыдущем шаге. Иначе они появятся в сети по умолчанию, которая есть в каждом каталоге.<br />Вот так выглядит команда создания подсети в зоне доступности ru-central1-a:<br />yc vpc subnet create \<br />&nbsp; --name my-subnet-1 \<br />&nbsp; --zone ru-central1-a \<br />&nbsp; --range 192.168.1.0/24 \<br />&nbsp; --network-name my-network <br />Где: name &mdash; имя подсети. zone &mdash; зона доступности. range &mdash; адресное пространство подсети. network-name &mdash; имя сети, в которой создаётся подсеть.<br />Создайте две другие подсети сами.<br />yc vpc subnet create \<br />&nbsp; --name my-subnet-2 \<br />&nbsp; --zone ru-central1-b \<br />&nbsp; --range 192.168.2.0/24 \<br />&nbsp; --network-name my-network<br />yc vpc subnet create \<br />&nbsp; --name my-subnet-3 \<br />&nbsp; --zone ru-central1-c \<br />&nbsp; --range 192.168.3.0/24 \<br />&nbsp; --network-name my-network <br />Осталось создать три ВМ в нужных зонах доступности и привязать к ним подсети.<br />Пусть машины работают под управлением ОС Ubuntu 20.04 LTS, имеют диски объёмом 30 Гб, 4 Гб оперативной памяти и два виртуальных процессорных ядра.<br />ВМ могут создаваться долго, поэтому запускайте команды в асинхронном режиме.<br />Пример команды для первой ВМ:<br />yc compute instance create \<br />&nbsp; --name my-instance-1 \<br />&nbsp; --hostname my-instance-1 \<br />&nbsp; --zone ru-central1-a \<br />&nbsp; --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \<br />&nbsp; --image-folder-id standard-images \<br />&nbsp; --memory 4 --cores 2 --core-fraction 100 \<br />&nbsp; --network-interface subnet-name=my-subnet-1,nat-ip-version=ipv4 \<br />&nbsp; --async <br />Посмотрите внимательно на все параметры. Подумайте, какой из них за что отвечает.<br />Создайте две другие машины сами.<br />yc compute instance create \<br />&nbsp; --name my-instance-2 \<br />&nbsp; --hostname my-instance-2 \<br />&nbsp; --zone ru-central1-b \<br />&nbsp; --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \<br />&nbsp; --image-folder-id standard-images \<br />&nbsp; --memory 4 --cores 2 --core-fraction 100 \<br />&nbsp; --network-interface subnet-name=my-subnet-2,nat-ip-version=ipv4 \<br />&nbsp; --async<br />&nbsp;yc compute instance create \<br />&nbsp; --name my-instance-3 \<br />&nbsp; --hostname my-instance-3 \<br />&nbsp; --zone ru-central1-c \<br />&nbsp; --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \<br />&nbsp; --image-folder-id standard-images \<br />&nbsp; --memory 4 --cores 2 --core-fraction 100 \<br />&nbsp; --network-interface subnet-name=my-subnet-3,nat-ip-version=ipv4 \<br />&nbsp; --async <br />После выполнения каждой команды благодаря флагу --async вы получите идентификатор операции и ее описание в виде:<br />id: c9q9v4bsn1hs9api4b13<br />description: Create instance<br />created_at: "2021-03-01T03:23:00.079888Z"<br />created_by: aje8s4vd4pp7cduq2o4k<br />modified_at: "2022-07-29T09:14:53.567744154Z"<br />metadata:<br />&nbsp; '@type': type.googleapis.com/yandex.cloud.compute.v1.CreateInstanceMetadata<br />&nbsp; instance_id: fhmepiaciq5l9slqid3k <br />Проследите за статусом одной из операций, используя ее идентификатор.<br />Проверьте синтаксис команды.<br />yc operation get &lt;идентификатор_операции&gt; <br />Дождитесь, пока операция завершится. Используйте для этого команду wait.<br />Проверьте синтаксис команды<br />yc operation wait &lt;идентификатор_операции&gt; <br />Убедитесь, что ВМ созданы. Для этого выведите их список.<br />yc compute instance list <br />По умолчанию список выдается в виде таблицы:<br />+----------------------+---------------+---------------+---------+---------------+--------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; ZONE ID&nbsp;&nbsp;&nbsp; | STATUS&nbsp; |&nbsp; EXTERNAL IP&nbsp; | INTERNAL IP&nbsp; |<br />+----------------------+---------------+---------------+---------+---------------+--------------+<br />| ef34r4fs8dsva3qtsivs | my-instance-3 | ru-central1-c | RUNNING | 51.250.44.130 | 192.168.3.34 |<br />| epdj7u79isrolup3vfo8 | my-instance-2 | ru-central1-b | RUNNING | 158.160.6.249 | 192.168.2.28 |<br />| fhmepiaciq5l9slqid3k | my-instance-1 | ru-central1-a | RUNNING | 62.84.126.39&nbsp; | 192.168.1.30 |<br />+----------------------+---------------+---------------+---------+---------------+--------------+ <br />Список можно вывести в формате YAML или JSON (эта возможность пригодится вам на следующих уроках):<br />yc compute instance list --format json <br />Список в формате JSON содержит больше информации, чем таблица.<br />Посмотрите результат<br />[<br />&nbsp; {<br />&nbsp;&nbsp;&nbsp; "id": "ef3rutmaas72bsujcja7",<br />&nbsp;&nbsp;&nbsp; "folder_id": "b1gfdbij3ijgopgqv9m9",<br />&nbsp;&nbsp;&nbsp; "created_at": "2021-06-21T12:41:10Z",<br />&nbsp;&nbsp;&nbsp; "name": "my-instance-3",<br />&nbsp;&nbsp;&nbsp; "zone_id": "ru-central1-c",<br />&nbsp;&nbsp;&nbsp; "platform_id": "standard-v2",<br />&nbsp;&nbsp;&nbsp; "resources": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "memory": "4294967296",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "cores": "2",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "core_fraction": "100"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "status": "RUNNING",<br />&nbsp;&nbsp;&nbsp; "metadata_options": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "gce_http_endpoint": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "aws_v1_http_endpoint": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "gce_http_token": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "aws_v1_http_token": "ENABLED"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "boot_disk": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "mode": "READ_WRITE",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "device_name": "ef3v2lor1u4pfn3ce1al",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "auto_delete": true,<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "disk_id": "ef3v2lor1u4pfn3ce1al"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "network_interfaces": [<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "index": "0",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "mac_address": "d0:0d:1b:f7:6c:a5",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "subnet_id": "b0c4h992tbuodl5hudpu",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "primary_v4_address": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "address": "10.128.0.32",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "one_to_one_nat": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "address": "178.154.212.5",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ip_version": "IPV4"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; ],<br />&nbsp;&nbsp;&nbsp; "fqdn": "my-instance-3.ru-central1.internal",<br />&nbsp;&nbsp;&nbsp; "scheduling_policy": {},<br />&nbsp;&nbsp;&nbsp; "network_settings": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "type": "STANDARD"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "placement_policy": {}<br />&nbsp; },<br />&nbsp; {<br />&nbsp;&nbsp;&nbsp; "id": "epd928ffks7m8ssc4i3k",<br />&nbsp;&nbsp;&nbsp; "folder_id": "b1gfdbij3ijgopgqv9m9",<br />&nbsp;&nbsp;&nbsp; "created_at": "2021-06-21T12:40:35Z",<br />&nbsp;&nbsp;&nbsp; "name": "my-instance-2",<br />&nbsp;&nbsp;&nbsp; "zone_id": "ru-central1-b",<br />&nbsp;&nbsp;&nbsp; "platform_id": "standard-v2",<br />&nbsp;&nbsp;&nbsp; "resources": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "memory": "4294967296",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "cores": "2",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "core_fraction": "100"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "status": "RUNNING",<br />&nbsp;&nbsp;&nbsp; "metadata_options": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "gce_http_endpoint": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "aws_v1_http_endpoint": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "gce_http_token": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "aws_v1_http_token": "ENABLED"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "boot_disk": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "mode": "READ_WRITE",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "device_name": "epddf7t0ljn9i1jp2pbs",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "auto_delete": true,<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "disk_id": "epddf7t0ljn9i1jp2pbs"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "network_interfaces": [<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "index": "0",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "mac_address": "d0:0d:91:21:ef:a7",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "subnet_id": "e2l1fgq2fbhnp6b929t7",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "primary_v4_address": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "address": "10.129.0.9",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "one_to_one_nat": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "address": "84.201.176.134",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ip_version": "IPV4"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; ],<br />&nbsp;&nbsp;&nbsp; "fqdn": "my-instance-2.ru-central1.internal",<br />&nbsp;&nbsp;&nbsp; "scheduling_policy": {},<br />&nbsp;&nbsp;&nbsp; "network_settings": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "type": "STANDARD"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "placement_policy": {}<br />&nbsp; },<br />&nbsp; {<br />&nbsp;&nbsp;&nbsp; "id": "fhm1op9id0dc6bubfags",<br />&nbsp;&nbsp;&nbsp; "folder_id": "b1gfdbij3ijgopgqv9m9",<br />&nbsp;&nbsp;&nbsp; "created_at": "2021-06-21T12:39:43Z",<br />&nbsp;&nbsp;&nbsp; "name": "my-instance-1",<br />&nbsp;&nbsp;&nbsp; "zone_id": "ru-central1-a",<br />&nbsp;&nbsp;&nbsp; "platform_id": "standard-v2",<br />&nbsp;&nbsp;&nbsp; "resources": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "memory": "4294967296",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "cores": "2",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "core_fraction": "100"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "status": "RUNNING",<br />&nbsp;&nbsp;&nbsp; "metadata_options": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "gce_http_endpoint": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "aws_v1_http_endpoint": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "gce_http_token": "ENABLED",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "aws_v1_http_token": "ENABLED"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "boot_disk": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "mode": "READ_WRITE",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "device_name": "fhmms7r7ia4uteikv1to",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "auto_delete": true,<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "disk_id": "fhmms7r7ia4uteikv1to"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "network_interfaces": [<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "index": "0",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "mac_address": "d0:0d:1c:65:32:68",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "subnet_id": "e9bcvlanhbum9ggdvkh2",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "primary_v4_address": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "address": "10.130.0.6",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "one_to_one_nat": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "address": "178.154.225.167",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ip_version": "IPV4"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; ],<br />&nbsp;&nbsp;&nbsp; "fqdn": "my-instance-1.ru-central1.internal",<br />&nbsp;&nbsp;&nbsp; "scheduling_policy": {},<br />&nbsp;&nbsp;&nbsp; "network_settings": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "type": "STANDARD"<br />&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp; "placement_policy": {}<br />&nbsp; }<br />] <br />Чтобы избежать ненужных расходов, удалите три созданные ВМ (в следующих практических работах они не понадобятся).<br />yc compute instance delete my-instance-1 my-instance-2 my-instance-3 <br /><strong>Decision:</strong><br />$ yc vpc network create --name my-network<br />$ yc vpc subnet create \<br />&nbsp; --name my-subnet-1 \<br />&nbsp; --zone ru-central1-a \<br />&nbsp; --range 192.168.1.0/24 \<br />&nbsp; --network-name my-network<br />$ yc vpc subnet create \<br />&nbsp; --name my-subnet-2 \<br />&nbsp; --zone ru-central1-b \<br />&nbsp; --range 192.168.2.0/24 \<br />&nbsp; --network-name my-network<br />$ yc vpc subnet create \<br />&nbsp; --name my-subnet-3 \<br />&nbsp; --zone ru-central1-c \<br />&nbsp; --range 192.168.3.0/24 \<br />&nbsp; --network-name my-network<br />$ yc compute instance create \<br />&nbsp; --name my-instance-1 \<br />&nbsp; --hostname my-instance-1 \<br />&nbsp; --zone ru-central1-a \<br />&nbsp; --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \<br />&nbsp; --image-folder-id standard-images \<br />&nbsp; --memory 4 --cores 2 --core-fraction 100 \<br />&nbsp; --network-interface subnet-name=my-subnet-1,nat-ip-version=ipv4 \<br />&nbsp; --async<br />$ yc compute instance create \<br />&nbsp; --name my-instance-2 \<br />&nbsp; --hostname my-instance-2 \<br />&nbsp; --zone ru-central1-b \<br />&nbsp; --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \<br />&nbsp; --image-folder-id standard-images \<br />&nbsp; --memory 4 --cores 2 --core-fraction 100 \<br />&nbsp; --network-interface subnet-name=my-subnet-2,nat-ip-version=ipv4 \<br />&nbsp; --async<br />$ yc compute instance create \<br />&nbsp; --name my-instance-3 \<br />&nbsp; --hostname my-instance-3 \<br />&nbsp; --zone ru-central1-c \<br />&nbsp; --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \<br />&nbsp; --image-folder-id standard-images \<br />&nbsp; --memory 4 --cores 2 --core-fraction 100 \<br />&nbsp; --network-interface subnet-name=my-subnet-3,nat-ip-version=ipv4 \<br />&nbsp; --async <br />$ yc operation get ef3vcn9383adr1anudcg<br />$ yc operation get epdk80vb9jjjc3190s6a<br />$ yc operation get fhm0u5fo2bq3cl2gagpr<br />$ yc operation wait fhm0u5fo2bq3cl2gagpr<br />$ yc compute instance list<br />$ yc compute instance list --format json<br />$ yc compute instance delete my-instance-1 my-instance-2 my-instance-3<br /><strong>Task:</strong><br />Использование файлов спецификаций<br />В этой практической работе вы создадите, обновите и удалите группу ВМ.<br />Вы уже убедились, что создать даже одну ВМ через yc непросто: нужно установить много разных параметров. Создание группы ВМ требует ещё больше параметров. <br />Чтобы не указывать их все в командной строке, конфигурацию описывают в файле, который используют при создании группы. Такой файл называется спецификацией. <br />Использование спецификаций &mdash; это первый шаг в освоении подхода Infrastructure as Code (IaC), который мы будем применять на следующих уроках.<br />Спецификации пишутся в разных форматах. Для группы ВМ используется язык YAML. Если вы не знакомы с ним &mdash; ничего страшного. <br /><strong>Decision:</strong><br />Часть 1. Создание Instance Group. Для разворачивания группы ВМ потребуется сеть. Если сети ещё нет, создайте её. Посмотрите информацию об имеющихся сетях.<br />yc vpc network list<br />Сохраните идентификатор сети, он понадобиться нам в дальнейшем.<br />По умолчанию все операции в Instance Groups выполняются от имени сервисного аккаунта c ролью editor на каталог. Если сервисного аккаунта нет, то тоже создайте его и назначьте эту роль.<br />Посмотрите информацию об имеющихся сервисных аккаунтах.<br />yc iam service-account list<br />Сохраните идентификатор сервисного аккаунта, он понадобится нам в дальнейшем.<br />Для создания группы необходимо подготовить её спецификацию. Создайте в любом текстовом редакторе файл с расширением yaml, например specification.yaml.<br />Обратите внимание: в формате YAML важны отступы слева. Даже если текст правильный, но отступы не соблюдены, при выполнении спецификации возникнут ошибки.<br />Сначала внесите информацию о группе. Пусть группа называется my-group. Укажите идентификатор сервисного аккаунта, от имени которого будете работать (см. шаг 2).<br />Идентификаторы ресурсов уникальны. Копируя команды из текста урока, не забывайте подставлять свои идентификаторы.<br />name: my-group<br />service_account_id: &lt;идентификатор_сервисного_аккаунта&gt;<br />Наша группа будет содержать три одинаковые ВМ. Машины создадим из публичного образа Ubuntu 18.04 LTS (возьмём не последнюю версию, чтобы потренироваться обновлять ВМ). Узнайте идентификатор образа с помощью команды:<br />yc compute image list --folder-id standard-images<br />В столбце FAMILY найдите ubuntu-1804-lts, в столбце ID будет указан нужный идентификатор.<br />Опишите в спецификации ВМ. Это раздел instance_template.<br />Пусть каждая машина использует платформу Intel Broadwell (посмотрите поддерживаемые платформы в документации Yandex Compute Cloud),&nbsp; имеет 2 Гб оперативной памяти и два процессорных ядра.<br />&nbsp;instance_template:<br />&nbsp;&nbsp;&nbsp;&nbsp; platform_id: standard-v1<br />&nbsp;&nbsp;&nbsp;&nbsp; resources_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: 2g<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cores: 2<br />Добавьте описание загрузочного диска. Он будет использоваться на чтение и запись (режим READ_WRITE). Укажите идентификатор образа, который получили на шаге 5. Выделите сетевой HDD объёмом 32 Гб.<br />&nbsp;&nbsp;&nbsp;&nbsp; boot_disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mode: READ_WRITE<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id: &lt;идентификатор_образа&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; type_id: network-hdd<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 32g<br />Теперь опишите сеть: идентификатор сети из каталога по умолчанию (см. шаг 1). Задайте публичный IP-адрес, чтобы к ВМ можно было обращаться извне.<br />&nbsp;&nbsp;&nbsp;&nbsp; network_interface_specs:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - network_id: &lt;идентификатор_сети&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}<br />В политике планирования укажите, что машина не прерываемая.<br />&nbsp;&nbsp;&nbsp;&nbsp; scheduling_policy:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; preemptible: false<br />В политике развертывания (раздел deploy policy) укажите, что в каждый момент времени может быть неработоспособной только одна машина, не больше. Запретите увеличивать число ВМ, т. е. создавать больше трех машин одновременно. Мы чуть подробнее разберём эти настройки, когда будем обновлять ВМ в группе.<br />&nbsp;&nbsp;&nbsp;&nbsp; deploy_policy:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; max_unavailable: 1<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; max_expansion: 0<br />Мы создаем группу фиксированного размера из трёх ВМ. Укажите это в политике масштабирования (раздел scale_policy):<br />&nbsp;&nbsp;&nbsp; scale_policy:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fixed_scale:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 3 <br />Наконец, в политике распределения машин по зонам (раздел allocation_policy) укажите, что будет использоваться зона ru-central1-a. Мы делаем это для простоты. Лучше распределять ВМ группы по разным зонам доступности &mdash; это позволит пережить краткие сбои или выход зоны из строя.<br />&nbsp;&nbsp;&nbsp; allocation_policy:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; zones:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-a <br />Для балансировщика нагрузки (раздел load_balancer_spec) укажите целевую группу, к которой он будет привязан (это мы рассмотрим чуть ниже).<br />&nbsp;&nbsp;&nbsp; load_balancer_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_group_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: my-target-group <br />Нашей спецификации уже достаточно, чтобы создать группу ВМ. Но на эти машины не будет установлено никакого ПО, только операционная система из публичного образа. Если не менять конфигурацию, то после создания ВМ вам придётся устанавливать программы вручную.<br />Чтобы сэкономить время и сократить число ошибок, давайте максимально автоматизируем создание ВМ, включая установку ПО. Для этого добавим в конфигурацию машины секцию, где будут вызываться команды установки программ. В этой же секции можно описать создание пользователей, но мы этого делать не будем, так как заходить на ВМ не планируем.<br />Установим на машины веб-сервер NGINX и на веб-странице index.nginx-debian.html, которая создается по умолчанию и выводит приветственное сообщение &laquo;Welcome to nginx&raquo;, заменим слово nginx идентификатором активной ВМ и версией ОС. Поскольку мы подключим балансировщик нагрузки, идентификатор активной ВМ будет различаться для разных пользователей. Это и позволит нам убедиться в том, что балансировщик работает.<br />Для установки ПО используйте cloud-init &mdash; пакет, выполняющий команды на ВМ при первом запуске. Вы узнали о нём из курса о ВМ. Команды опишите в блоке конфигурации #cloud-config. Примеры команд смотрите в документации cloud-init.<br />Содержимое #cloud-config описывается в разделе instance_template в секции metadata:<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; user-data: |-<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #cloud-config<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; package_update: true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; runcmd:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [apt-get, install, -y, nginx ]<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html'] <br />Спецификация готова. Вот ее полный текст. Помните, что в формате YAML важно соблюдать отступы слева.<br />name: my-group<br />service_account_id: ajeu495h1s9tn1rorulb<br />instance_template:<br />&nbsp;&nbsp;&nbsp; platform_id: standard-v1<br />&nbsp;&nbsp;&nbsp; resources_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: 2g<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cores: 2<br />&nbsp;&nbsp;&nbsp; boot_disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mode: READ_WRITE<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id: fd8fosbegvnhj5haiuoq <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; type_id: network-hdd<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 32g<br />&nbsp;&nbsp;&nbsp; network_interface_specs:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - network_id: enpnr4onfs6ihtoao32u<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}<br />&nbsp;&nbsp;&nbsp; scheduling_policy:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; preemptible: false<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; user-data: |-<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #cloud-config<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; package_update: true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; runcmd:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [ apt-get, install, -y, nginx ]<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']<br />deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 1<br />&nbsp;&nbsp;&nbsp; max_expansion: 0<br />scale_policy:<br />&nbsp;&nbsp;&nbsp; fixed_scale:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 3<br />allocation_policy:<br />&nbsp;&nbsp;&nbsp; zones:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-a<br />load_balancer_spec:<br />&nbsp;&nbsp;&nbsp; target_group_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: my-target-group <br />Теперь создайте группу ВМ по подготовленной спецификации. Уточните синтаксис команды сами:<br />yc compute instance-group --help <br />Проверить синтаксис команды<br />yc compute instance-group create --file &lt;путь_к_файлу_specification.yaml&gt; &nbsp;<br />Для тренировки можете вызвать эту команду в асинхронном режиме, а затем проверить её статус и дождаться завершения.<br />Убедитесь, что группа создана, в веб-консоли или выведя список групп с помощью yc.<br />yc compute instance-group list <br />В списке вы должны увидеть свою группу машин my-group:<br />+----------------------+------------+------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp; | SIZE |<br />+----------------------+------------+------+<br />| amc65sbgfqeqf00m02sc | my-group&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 3 |<br />+----------------------+------------+------+ <br />Часть 2. Балансировщик<br />Создайте балансировщик my-load-balancer. Посмотрите, какие параметры должны быть у соответствующей команды:<br />yc load-balancer network-load-balancer create --help <br />В выводе справки обратите внимание, что при создании балансировщика можно сразу создать и обработчик входящего трафика (параметр --listener).<br />Формат параметра --listener достаточно хитрый: в нём можно указать сразу несколько подпараметров через запятую:<br />...<br />--listener name=my-listener,external-ip-version=ipv4,port=80<br />... <br />Помимо имени обработчика, здесь указывается версия IP-протокола и порт, на котором балансировщик будет принимать трафик.<br />Проверить синтаксис команды<br />yc load-balancer network-load-balancer create \<br />&nbsp; --region-id ru-central1 \<br />&nbsp; --name my-load-balancer \<br />&nbsp; --listener name=my-listener,external-ip-version=ipv4,port=80 <br />Затем подключите к балансировщику целевую группу (команда attach-target-group). Вам понадобится идентификатор целевой группы. Чтобы узнать его, запросите с помощью yc список доступных целевых групп и выберите ту, которую вы указали в спецификации specification.yaml. <br />Проверить синтаксис команды<br />yc load-balancer target-group list <br />Целевая группа также подключается с помощью нескольких подпараметров, которые соответствуют настройкам в консоли управления (их вы изучали на первом курсе). Для целевой группы укажите такие параметры:<br />&nbsp;&nbsp;&nbsp; target-group-id &mdash; идентификатор группы;<br />&nbsp;&nbsp;&nbsp; healthcheck-name, healthcheck-interval, healthcheck-timeout, healthcheck-unhealthythreshold, healthcheck-healthythreshold, healthcheck-http-port &mdash; параметры проверки состояния (см. документацию). Эти параметры аналогичны тем, что задаются в консоли управления при создании балансировщика. Вы изучали их в первом курсе.<br />Укажите 80-й порт, на котором запущен NGINX.<br />Проверить синтаксис команды<br />yc load-balancer network-load-balancer attach-target-group my-load-balancer \ <br />&nbsp; --target-group target-group-id=&lt;идентификатор целевой группы&gt;,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80<br />Можно не выполнять две команды (создание балансировщика и подключение целевой группы) по очереди, а одной командой create создать балансировщик с привязанной целевой группой.<br />Убедитесь, что балансировщик создан, а целевая группа подключена через консоль управления или с помощью yc.<br />Часть 3. Доступ к машинам группы<br />Проверьте состояние машин группы. Для этого запросите список машин и дождитесь статуса HEALTHY.<br />yc load-balancer network-load-balancer target-states my-load-balancer \<br />&nbsp;&nbsp;&nbsp; --target-group-id &lt;идентификатор_целевой_группы&gt; <br />Теперь откройте в браузере страницу балансировщика. IP-адрес балансировщика вы можете узнать с помощью консоли управления или yc.<br />На странице вы увидите приветственное сообщение и в нём идентификатор одной из машин.<br />Часть 4. Обновление Instance Group<br />При создании на ВМ группы была установлена ОС Ubuntu 18.04 LTS. Теперь обновите её до Ubuntu 20.04 LTS (ubuntu-2004-lts в столбце FAMILY). Ещё раз посмотрите список доступных образов (см. часть 1) и в файле спецификации specification.yaml измените параметр image_id.<br />...<br />boot_disk_spec:<br />&nbsp;&nbsp; mode: READ_WRITE<br />&nbsp;&nbsp; disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id: &lt;идентификатор_образа&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; type_id: network-hdd<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 32g<br />... <br />Теперь запустите обновление группы с изменённым файлом спецификации.<br />Проверить синтаксис команды<br />yc compute instance-group update \<br />&nbsp; --name my-group \<br />&nbsp; --file &lt;путь_к_файлу_specification.yaml&gt; <br />Группа будет обновляться постепенно: когда одна машина из группы удаляется, ей на замену создаётся новая. Общее число машин в группе не увеличится. Именно такую политику обновления мы задали в файле спецификации (см. часть 1):<br />...<br />deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 1<br />&nbsp;&nbsp;&nbsp; max_expansion: 0<br />... <br />Есть и другой режим обновления: сначала в группу добавляется ВМ с новой конфигурацией, а затем отключается старая машина. Это повторяется, пока не обновятся все машины. Такому режиму соответствовала бы другая конфигурация:<br />...<br />deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 0<br />&nbsp;&nbsp;&nbsp; max_expansion: 1<br />... <br />Убедитесь, что машины обновились. На приветственной странице должна выводиться новая версия ОС.<br />Часть 5. Удаление машины из группы<br />На приветственной странице балансировщика посмотрите имя активной машины и попробуйте удалить ее. Убедитесь, что приветственная страница остаётся доступна всё время: балансировщик переключит трафик на другую машину группы. А Yandex Cloud тем временем пересоздаст удалённую машину.<br />Проверить синтаксис команды<br />yc compute instance delete &lt;имя_ВМ&gt; <br />Часть 6. Удаление Instance Group<br />Теперь удалите группу и балансировщик командами yc.<br />Проверить синтаксис команд<br />yc compute instance-group delete --name my-group<br />yc load-balancer network-load-balancer delete --name my-load-balancer <br />Кстати, ключевой параметр --name можно и не писать. Достаточно указать имя группы или балансировщика.<br />Убедитесь, что группы и балансировщика больше нет, через консоль управления или с помощью yc. <br /><strong>Decision:</strong><br />$ yc vpc network list<br />$ yc iam service-account list<br />$ vim specification.yaml<br />$ cat specification.yaml<br />name: my-group<br />service_account_id: ajeq7kga9ms7bhup4gbe<br />$ yc compute image list --folder-id standard-images<br />$ vim specification.yaml<br />$ cat specification.yaml<br />name: my-group<br />service_account_id: ajeq7kga9ms7bhup4gbe<br />instance_template:<br />&nbsp;&nbsp;&nbsp; platform_id: standard-v1<br />&nbsp;&nbsp;&nbsp; resources_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: 2g<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cores: 2<br />&nbsp;&nbsp;&nbsp; boot_disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mode: READ_WRITE<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id: fd8k6joqhuk8ts8eb1ao <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; type_id: network-hdd<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 32g<br />&nbsp;&nbsp;&nbsp; network_interface_specs:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - network_id: enpboucd6803lg6jspnh<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}<br />&nbsp;&nbsp;&nbsp; scheduling_policy:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; preemptible: false<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; user-data: |-<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #cloud-config<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; package_update: true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; runcmd:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [ apt-get, install, -y, nginx ]<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']<br />deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 1<br />&nbsp;&nbsp;&nbsp; max_expansion: 0<br />scale_policy:<br />&nbsp;&nbsp;&nbsp; fixed_scale:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 3<br />allocation_policy:<br />&nbsp;&nbsp;&nbsp; zones:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-a<br />load_balancer_spec:<br />&nbsp;&nbsp;&nbsp; target_group_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: my-target-group<br />$ yc compute instance-group --help <br />$ yc compute instance-group create --file /YOUR-DIR/specification.yaml<br />$ yc compute instance-group list<br />$ yc load-balancer network-load-balancer create --help<br />$ yc load-balancer network-load-balancer create \<br />&nbsp; --region-id ru-central1 \<br />&nbsp; --name my-load-balancer \<br />&nbsp; --listener name=my-listener,external-ip-version=ipv4,port=80 <br />$ yc load-balancer target-group list<br />$ yc load-balancer network-load-balancer attach-target-group my-load-balancer \ <br />--target-group target-group-id=enp3edjdaoot0v64qth0,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80<br />$ yc load-balancer network-load-balancer target-states my-load-balancer \<br />--target-group-id enp3edjdaoot0v64qth0<br />$ vim specification.yaml<br />$ cat specification.yaml<br />...<br />boot_disk_spec:<br />&nbsp;&nbsp; mode: READ_WRITE<br />&nbsp;&nbsp; disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id: &lt;идентификатор_образа&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; type_id: network-hdd<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 32g<br />...<br />$ yc compute instance-group update \<br />&nbsp; --name my-group \<br />&nbsp; --file /YOUR-DIR/specification.yaml<br />$ yc compute instance-group delete --name my-group<br />$ yc load-balancer network-load-balancer delete --name my-load-balancer<br /><strong>Task:</strong><br />Создаём образ виртуальной машины.<br />В этой практической работе вы установите Packer, подготовите с его помощью образ, а затем создадите из образа виртуальную машину.<br /><strong>Decision:</strong><br />Установите Packer, если ещё не сделали это на предыдущем уроке. Он поддерживает все популярные операционные системы &mdash; Windows, macOS, Linux и FreeBSD.<br />Скачать дистрибутив Packer для вашей ОС также можно с зеркала Yandex Cloud.<br />Подготовьте файл в формате HCL со спецификацией образа, например my-ubuntu-nginx.pkr.hcl.<br />При создании файла опирайтесь на документацию Packer.<br />В качестве примера можете взять спецификацию из предыдущего урока:<br />&nbsp;source "yandex" "ubuntu-nginx" {<br />&nbsp;&nbsp; token&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "&lt;OAuth-токен&gt;"<br />&nbsp;&nbsp; folder_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "&lt;идентификатор_каталога&gt;"<br />&nbsp;&nbsp; source_image_family = "ubuntu-2004-lts"<br />&nbsp;&nbsp; ssh_username&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ubuntu"<br />&nbsp;&nbsp; use_ipv4_nat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "true"<br />&nbsp;&nbsp; image_description&nbsp;&nbsp; = "my custom ubuntu with nginx"<br />&nbsp;&nbsp; image_family&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ubuntu-2004-lts"<br />&nbsp;&nbsp; image_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "my-ubuntu-nginx"<br />&nbsp;&nbsp; subnet_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "&lt;идентификатор_подсети&gt;"<br />&nbsp;&nbsp; disk_type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "network-ssd"<br />&nbsp;&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp;}<br />&nbsp;build {<br />&nbsp;&nbsp; sources = ["source.yandex.ubuntu-nginx"]<br />&nbsp;&nbsp; provisioner "shell" {<br />&nbsp;&nbsp;&nbsp;&nbsp; inline = ["sudo apt-get update -y",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "sudo apt-get install -y nginx",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "sudo systemctl enable nginx.service"]<br />&nbsp;&nbsp; }<br />&nbsp;}<br />Не забудьте подставить в спецификацию идентификаторы своего каталога и подсети (подсеть должна быть в той же зоне доступности, которая указана в параметре zone). Также укажите свой OAuth-токен (или воспользуйтесь переменной окружения YC_TOKEN при сборке образа).<br />Теперь создайте образ ВМ на основе файла спецификации:<br />&nbsp;packer build &lt;путь_к_файлу_my-ubuntu-nginx.pkr.hcl&gt;<br />После того как команда отработает, убедитесь, что образ появился в каталоге. Для этого в консоли управления перейдите в сервис Compute Cloud. Найдите образ на вкладке Образы.<br />Перейдите на вкладку Виртуальные машины и начните создавать ВМ.<br />Раньше для создания загрузочного диска вы выбирали один из публичных образов, например Ubuntu 20.04. Теперь вместо этого переключитесь на вкладку Пользовательские. Нажмите кнопку Выбрать и в открывшемся окне переключитесь на вкладку Образ.<br />Выберите созданный образ и нажмите Применить.<br />Из образа создастся загрузочный диск.<br />Завершите создание ВМ.<br />Проверьте ВМ: введите её IP-адрес в адресную строку браузера. Убедитесь, что веб-сервер работает.<br />Удалите ВМ: на следующих уроках она не понадобится. А вот образ удалять не стоит.<br /><strong>Decision:</strong><br />$ sudo apt-get install packer<br />$ vim my-ubuntu-nginx.pkr.hcl<br />$ cat my-ubuntu-nginx.pkr.hcl<br />source "yandex" "ubuntu-nginx" {<br />&nbsp;&nbsp; token&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "YOUR-KEY"<br />&nbsp;&nbsp; folder_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "YOUR-ID"<br />&nbsp;&nbsp; source_image_family = "ubuntu-2004-lts"<br />&nbsp;&nbsp; ssh_username&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ubuntu"<br />&nbsp;&nbsp; use_ipv4_nat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "true"<br />&nbsp;&nbsp; image_description&nbsp;&nbsp; = "my custom ubuntu with nginx"<br />&nbsp;&nbsp; image_family&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ubuntu-2004-lts"<br />&nbsp;&nbsp; image_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "my-ubuntu-nginx"<br />&nbsp;&nbsp; subnet_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "enpboucd6803lg6jspnh"<br />&nbsp;&nbsp; disk_type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "network-ssd"<br />&nbsp;&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />}<br />build {<br />&nbsp;&nbsp; sources = ["source.yandex.ubuntu-nginx"]<br />&nbsp;&nbsp; provisioner "shell" {<br />&nbsp;&nbsp;&nbsp;&nbsp; inline = ["sudo apt-get update -y",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "sudo apt-get install -y nginx",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "sudo systemctl enable nginx.service"]<br />&nbsp;&nbsp; &nbsp;}<br />}<br />$ packer build /YOUR-DIR/my-ubuntu-nginx.pkr.hcl<br /><strong>Task:</strong><br />Создаём виртуальную машину из образа и базу данных.<br />В этой практической работе вы установите Terraform и подготовите спецификацию, с помощью которой создадите виртуальную машину, а затем управляемую базу данных.<br /><strong>Decision:</strong><br />Установите Terraform. Дистрибутив для вашей платформы можно скачать из зеркала. После загрузки добавьте путь к папке, в которой находится исполняемый файл, в переменную PATH.<br />Настройте провайдер. Если раньше у вас был настроен провайдер из реестра Hashicorp, сохраните его настройки:<br />mv ~/.terraformrc ~/.terraformrc.old<br />Укажите источник, из которого будет устанавливаться провайдер.<br />Откройте файл конфигурации Terraform CLI:<br />nano ~/.terraformrc<br />Добавьте в него следующий блок:<br />provider_installation {<br />&nbsp; network_mirror {<br />&nbsp;&nbsp;&nbsp; url = "https://terraform-mirror.yandexcloud.net/"<br />&nbsp;&nbsp;&nbsp; include = ["registry.terraform.io/*/*"]<br />&nbsp; }<br />&nbsp; direct {<br />&nbsp;&nbsp;&nbsp; exclude = ["registry.terraform.io/*/*"]<br />&nbsp; }<br />}<br />В начале конфигурационного файла .tf добавьте следующие блоки:<br />terraform {<br />&nbsp; required_providers {<br />&nbsp;&nbsp;&nbsp; yandex = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; source = "yandex-cloud/yandex"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp; required_version = "&gt;= 0.13"<br />}<br />provider "yandex" {<br />&nbsp; zone = "&lt;зона доступности по умолчанию&gt;"<br />}<br />Где: source &mdash; глобальный адрес источника провайдера. required_version &mdash; минимальная версия Terraform, с которой совместим провайдер. provider &mdash; название провайдера. zone &mdash; зона доступности, в которой по умолчанию будут создаваться все облачные ресурсы.<br />Выполните команду terraform init в папке с конфигурационным файлом .tf. Эта команда инициализирует провайдеров, указанных в конфигурационных файлах, и позволяет работать с ресурсами и источниками данных провайдера.<br />Если провайдер не установился, создайте обращение в поддержку с именем и версией провайдера.<br />Если вы использовали файл .terraform.lock.hcl, то перед инициализацией выполните команду terraform providers lock, указав адрес зеркала, откуда будет загружаться провайдер, и платформы, на которых будет использоваться конфигурация:<br />terraform providers lock -net-mirror=https://terraform-mirror.yandexcloud.net -platform=linux_amd64 -platform=darwin_arm64 yandex-cloud/yandex<br />Если вы использовали модули, то сначала выполните terraform init, затем удалите lock-файл, а затем выполните команду terraform providers lock.<br />Создайте файл спецификации my-config.tf и укажите в нём Yandex Cloud в качестве провайдера.<br />&nbsp;terraform {<br />&nbsp;&nbsp; required_providers {<br />&nbsp;&nbsp;&nbsp;&nbsp; yandex = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; source = "yandex-cloud/yandex"<br />&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp; }<br />&nbsp;}<br />&nbsp;provider "yandex" {<br />&nbsp;&nbsp; token&nbsp; =&nbsp; "&lt;OAuth-токен&gt;"<br />&nbsp;&nbsp; cloud_id&nbsp; = "&lt;идентификатор_облака&gt;"<br />&nbsp;&nbsp; folder_id = "&lt;идентификатор_каталога&gt;"<br />&nbsp;&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "&lt;зона_доступности_по_умолчанию&gt;"<br />&nbsp;}<br />Далее мы будем считать, что в качестве зоны доступности по умолчанию выбрана ru-central1-a.<br />Добавьте в файл блок, описывающий создание ВМ. Его сложно написать с нуля, поэтому опирайтесь на пример из документации. Чтобы вам было проще опознать в консоли управления объекты, созданные по этой спецификации, указывайте уникальные имена для ВМ, сети и подсети, а не оставляйте имена по умолчанию (default).<br />Для создания ВМ используйте образ, созданный с помощью Packer в предыдущей практической работе.<br />Можно использовать переменные в спецификации Terraform и передавать в них разные значения при запуске команд. Например, если сделать переменную для идентификатора образа image-id, тогда с помощью одного и того же файла спецификации вы сможете создавать ВМ с разным наполнением.<br />Переменные Terraform хранятся в файлах с расширением .tfvars. Создайте файл my-variables.tfvars и укажите в нём идентификатор своего образа Packer (узнайте идентификатор с помощью команды yc compute image list):<br />&nbsp;image-id = "&lt;идентификатор_образа&gt;"<br />В файле спецификации my-config.tf объявите эту переменную (ключевое слово variable). Тогда в секции, где описываются настройки ВМ, вы сможете обратиться к переменной как var.image-id:<br />&nbsp;...<br />&nbsp;variable "image-id" {<br />&nbsp;&nbsp;&nbsp;&nbsp; type = string<br />&nbsp;}<br />&nbsp;resource "yandex_compute_instance" "vm-1" {<br />&nbsp;...&nbsp; &nbsp;<br />&nbsp;&nbsp;&nbsp;&nbsp; boot_disk {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; initialize_params {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id = var.image-id<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;...<br />Скорректируйте описание для сети и подсети.<br />Для сети достаточно указать имя:<br />&nbsp; resource "yandex_vpc_network" "network-1" {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name = "from-terraform-network"<br />&nbsp; }<br />Для подсети укажите зону доступности и сеть, а также внутренние IP-адреса, уникальные в рамках сети. Используйте адреса из адресного пространства 10.0.0.0/16.<br />&nbsp; resource "yandex_vpc_subnet" "subnet-1" {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "from-terraform-subnet"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; network_id&nbsp;&nbsp;&nbsp;&nbsp; = "${yandex_vpc_network.network-1.id}"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v4_cidr_blocks = ["10.2.0.0/16"]<br />&nbsp; }<br />Проверьте синтаксис спецификации:<br />variable "image-id" {<br />&nbsp; type = string<br />}<br />resource "yandex_compute_instance" "vm-1" {<br />&nbsp; name = "from-terraform-vm"<br />&nbsp; platform_id = "standard-v1"<br />&nbsp; zone = "ru-central1-a"<br />&nbsp; resources {<br />&nbsp;&nbsp;&nbsp; cores&nbsp; = 2<br />&nbsp;&nbsp;&nbsp; memory = 2<br />&nbsp; }<br />&nbsp; boot_disk {<br />&nbsp;&nbsp;&nbsp; initialize_params {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id = var.image-id<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp; network_interface {<br />&nbsp;&nbsp;&nbsp; subnet_id = yandex_vpc_subnet.subnet-1.id<br />&nbsp;&nbsp;&nbsp; nat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = true<br />&nbsp; }<br />&nbsp; metadata = {<br />&nbsp;&nbsp;&nbsp; ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"<br />&nbsp; }<br />}<br />resource "yandex_vpc_network" "network-1" {<br />&nbsp; name = "from-terraform-network"<br />}<br />resource "yandex_vpc_subnet" "subnet-1" {<br />&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "from-terraform-subnet"<br />&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp; network_id&nbsp;&nbsp;&nbsp;&nbsp; = "${yandex_vpc_network.network-1.id}"<br />&nbsp; v4_cidr_blocks = ["10.2.0.0/16"]<br />}<br />output "internal_ip_address_vm_1" {<br />&nbsp; value = yandex_compute_instance.vm-1.network_interface.0.ip_address<br />}<br />output "external_ip_address_vm_1" {<br />&nbsp; value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address<br />}<br />Теперь попробуйте применить спецификацию. Перейдите в папку с файлом спецификации и выполните инициализацию.<br />&nbsp;terraform init<br />Если всё сделано верно, Terraform покажет сообщение:<br />&nbsp;...<br />&nbsp;Terraform has been successfully initialized!<br />&nbsp;...<br />Важно: выполняйте команды Terraform в папке, где находится файл спецификации.<br />Проверьте спецификацию с помощью команды terraform plan.<br />Terraform использует все файлы .tf из папки, в которой запущена команда. Поэтому название файла спецификации my-config.tf указывать не нужно: его Terraform подхватит и так.<br />Если файл с переменными называется стандартно (terraform.tfvars), его тоже можно не указывать при запуске команды. А если название файла нестандартное, то его нужно указывать:<br />&nbsp;terraform plan -var-file=my-variables.tfvars<br />Terraform выведет план: объекты, которые будут созданы, и т. п.:<br />&nbsp;...<br />&nbsp;Terraform will perform the following actions:<br />&nbsp;...<br />На самом деле необязательно помещать переменные в файл, их можно просто указывать при запуске команды. Поскольку у вас только одна переменная, это было бы несложно:<br />&nbsp;terraform plan -var="image-id=&lt;идентификатор_образа&gt;"<br />Создайте в облаке инфраструктуру по описанной вами спецификации. Выполните команду:<br />&nbsp;terraform apply -var-file=my-variables.tfvars<br />Terraform запросит подтверждение:<br />&nbsp;...<br />&nbsp;Do you want to perform these actions?<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Terraform will perform the actions described above.<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Only 'yes' will be accepted to approve.<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Enter a value:<br />В ответ введите yes.<br />Когда команда будет выполнена, вы увидите сообщение:<br />&nbsp; Apply complete! Resources: ... added, 0 changed, 0 destroyed.<br />&nbsp; Outputs:<br />&nbsp; external_ip_address_vm_1 = "84.201.133.49"<br />&nbsp; internal_ip_address_vm_1 = "10.2.0.24"<br />В консоли управления убедитесь, что ВМ создана. Откройте в браузере страницу с указанным IP-адресом и проверьте, доступна ли ВМ.<br />Как мы говорили на предыдущем уроке, Terraform хранит описание инфраструктуры в стейт-файлах. Посмотрите, как выглядит стейт-файл сейчас:<br />&nbsp;terraform state list<br />Вы увидите список объектов:<br />&nbsp;yandex_compute_instance.vm-1<br />&nbsp;yandex_vpc_network.network-1<br />&nbsp;yandex_vpc_subnet.subnet-1<br />Теперь добавьте в файл спецификации блок, описывающий создание кластера БД PostgreSQL. Подсказки ищите в справочнике ресурсов. Не забудьте заменить в спецификации имя подсети.<br />Проверьте синтаксис спецификации:<br />resource "yandex_mdb_postgresql_cluster" "postgres-1" {<br />&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "postgres-1"<br />&nbsp; environment = "PRESTABLE"<br />&nbsp; network_id&nbsp; = yandex_vpc_network.network-1.id<br />&nbsp; config {<br />&nbsp;&nbsp;&nbsp; version = 12<br />&nbsp;&nbsp;&nbsp; resources {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; resource_preset_id = "s2.micro"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_type_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "network-ssd"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 16<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; postgresql_config = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; max_connections&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 395<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; enable_parallel_hash&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vacuum_cleanup_index_scale_factor = 0.2<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; autovacuum_vacuum_scale_factor&nbsp;&nbsp;&nbsp; = 0.34<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; default_transaction_isolation&nbsp;&nbsp;&nbsp;&nbsp; = "TRANSACTION_ISOLATION_READ_COMMITTED"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; shared_preload_libraries&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp;&nbsp; database {<br />&nbsp;&nbsp;&nbsp; name&nbsp; = "postgres-1"<br />&nbsp;&nbsp;&nbsp; owner = "my-name"<br />&nbsp; }<br />&nbsp; user {<br />&nbsp;&nbsp;&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "my-name"<br />&nbsp;&nbsp;&nbsp; password&nbsp;&nbsp; = "Test1234"<br />&nbsp;&nbsp;&nbsp; conn_limit = 50<br />&nbsp;&nbsp;&nbsp; permission {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; database_name = "postgres-1"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; settings = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; default_transaction_isolation = "read committed"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log_min_duration_statement&nbsp;&nbsp;&nbsp; = 5000<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp; host {<br />&nbsp;&nbsp;&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp;&nbsp;&nbsp; subnet_id = yandex_vpc_subnet.subnet-1.id<br />&nbsp; }<br />}<br />Сохраните файл спецификации.<br />Проверьте синтаксис спецификации:<br />terraform {<br />&nbsp; required_providers {<br />&nbsp;&nbsp;&nbsp; yandex = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; source = "yandex-cloud/yandex"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />}<br />provider "yandex" {<br />&nbsp; token&nbsp; =&nbsp; "&lt;OAuth-токен&gt;"<br />&nbsp; cloud_id&nbsp; = "&lt;идентификатор_облака&gt;"<br />&nbsp; folder_id = "&lt;идентификатор_каталога&gt;"<br />&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />}<br />variable "image-id" {<br />&nbsp; type = string<br />}<br />resource "yandex_compute_instance" "vm-1" {<br />&nbsp; name = "from-terraform-vm"<br />&nbsp; platform_id = "standard-v1"<br />&nbsp; zone = "ru-central1-a"<br />&nbsp; resources {<br />&nbsp;&nbsp;&nbsp; cores&nbsp; = 2<br />&nbsp;&nbsp;&nbsp; memory = 2<br />&nbsp; }<br />&nbsp; boot_disk {<br />&nbsp;&nbsp;&nbsp; initialize_params {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id = var.image-id<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp; network_interface {<br />&nbsp;&nbsp;&nbsp; subnet_id = yandex_vpc_subnet.subnet-1.id<br />&nbsp;&nbsp;&nbsp; nat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = true<br />&nbsp; }<br />&nbsp; metadata = {<br />&nbsp;&nbsp;&nbsp; ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"<br />&nbsp; }<br />}<br />resource "yandex_vpc_network" "network-1" {<br />&nbsp; name = "from-terraform-network"<br />}<br />resource "yandex_vpc_subnet" "subnet-1" {<br />&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "from-terraform-subnet"<br />&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp; network_id&nbsp;&nbsp;&nbsp;&nbsp; = yandex_vpc_network.network-1.id<br />&nbsp; v4_cidr_blocks = ["10.2.0.0/16"]<br />}<br />resource "yandex_mdb_postgresql_cluster" "postgres-1" {<br />&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "postgres-1"<br />&nbsp; environment = "PRESTABLE"<br />&nbsp; network_id&nbsp; = yandex_vpc_network.network-1.id<br />&nbsp; config {<br />&nbsp;&nbsp;&nbsp; version = 12<br />&nbsp;&nbsp;&nbsp; resources {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; resource_preset_id = "s2.micro"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_type_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "network-ssd"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 16<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; postgresql_config = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; max_connections&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 395<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; enable_parallel_hash&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vacuum_cleanup_index_scale_factor = 0.2<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; autovacuum_vacuum_scale_factor&nbsp;&nbsp;&nbsp; = 0.34<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; default_transaction_isolation&nbsp;&nbsp;&nbsp;&nbsp; = "TRANSACTION_ISOLATION_READ_COMMITTED"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; shared_preload_libraries&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp; database {<br />&nbsp;&nbsp;&nbsp; name&nbsp; = "postgres-1"<br />&nbsp;&nbsp;&nbsp; owner = "my-name"<br />&nbsp; }<br />&nbsp; user {<br />&nbsp;&nbsp;&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "my-name"<br />&nbsp;&nbsp;&nbsp; password&nbsp;&nbsp; = "Test1234"<br />&nbsp;&nbsp;&nbsp; conn_limit = 50<br />&nbsp;&nbsp;&nbsp; permission {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; database_name = "postgres-1"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; settings = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; default_transaction_isolation = "read committed"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log_min_duration_statement&nbsp;&nbsp;&nbsp; = 5000<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp; host {<br />&nbsp;&nbsp;&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp;&nbsp;&nbsp; subnet_id = yandex_vpc_subnet.subnet-1.id<br />&nbsp; }<br />}<br />output "internal_ip_address_vm_1" {<br />&nbsp; value = yandex_compute_instance.vm-1.network_interface.0.ip_address<br />}<br />output "external_ip_address_vm_1" {<br />&nbsp; value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address<br />}<br />Теперь примените обновлённую спецификацию. В папке с файлом спецификации выполните команду terraform plan:<br />terraform plan -var-file=my-variables.tfvars <br />Если появляются сообщения об ошибках &mdash; исправьте ошибки и снова выполните команду.<br />Обновите инфраструктуру в соответствии с дополненной спецификацией командой terraform apply:<br />terraform apply -var-file=my-variables.tfvars <br />Поскольку спецификация теперь включает создание БД, команда может выполняться довольно долго (около 10 минут).<br />В консоли управления откройте раздел Managed Service for PostgreSQL и убедитесь, что кластер postgres-1 создан и имеет статус Alive.<br />Проверьте, как изменился стейт-файл:<br />terraform state list <br />В списке появился новый объект:<br />yandex_compute_instance.vm-1<br />yandex_mdb_postgresql_cluster.postgres-1<br />yandex_vpc_network.network-1<br />yandex_vpc_subnet.subnet-1 <br />Удалите инфраструктуру:<br />terraform destroy -var-file=my-variables.tfvars <br />В конце вы увидите сообщение о выполнении команды:<br />...<br />Destroy complete! Resources: 4 destroyed. <br />В консоли управления убедитесь, что объекты удалены.<br /><strong>Decision:</strong><br />$ wget https://hashicorp-releases.yandexcloud.net/terraform/1.6.5/terraform_1.6.5_linux_amd64.zip<br />$ unzip terraform_1.6.5_linux_amd64.zip <br />$ vim my-config.tf<br />$ cat my-config.tf<br />terraform {<br />&nbsp; required_providers {<br />&nbsp;&nbsp;&nbsp; yandex = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; source = "yandex-cloud/yandex"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />}<br />provider "yandex" {<br />&nbsp; token&nbsp; =&nbsp; "YOUR-TOKEN"<br />&nbsp; cloud_id&nbsp; = "YOUR-ID1"<br />&nbsp; folder_id = "YOUR-ID2"<br />&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />}<br />variable "image-id" {<br />&nbsp; type = string<br />}<br />resource "yandex_compute_instance" "vm-1" {<br />&nbsp; name = "from-terraform-vm"<br />&nbsp; platform_id = "standard-v1"<br />&nbsp; zone = "ru-central1-a"<br />&nbsp; resources {<br />&nbsp;&nbsp;&nbsp; cores&nbsp; = 2<br />&nbsp;&nbsp;&nbsp; memory = 2<br />&nbsp; }<br />&nbsp; boot_disk {<br />&nbsp;&nbsp;&nbsp; initialize_params {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id = var.image-id<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp; network_interface {<br />&nbsp;&nbsp;&nbsp; subnet_id = yandex_vpc_subnet.subnet-1.id<br />&nbsp;&nbsp;&nbsp; nat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = true<br />&nbsp; }<br />&nbsp; metadata = {<br />&nbsp;&nbsp;&nbsp; ssh-keys = "ubuntu:${file("/YOUR-DIR/YOUR-KEY.pub")}"<br />&nbsp; }<br />}<br />resource "yandex_vpc_network" "network-1" {<br />&nbsp; name = "from-terraform-network"<br />}<br />resource "yandex_vpc_subnet" "subnet-1" {<br />&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "from-terraform-subnet"<br />&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp; network_id&nbsp;&nbsp;&nbsp;&nbsp; = "${yandex_vpc_network.network-1.id}"<br />&nbsp; v4_cidr_blocks = ["10.2.0.0/16"]<br />}<br />output "internal_ip_address_vm_1" {<br />&nbsp; value = yandex_compute_instance.vm-1.network_interface.0.ip_address<br />}<br />output "external_ip_address_vm_1" {<br />&nbsp; value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address<br />}<br />$ vim my-variables.tfvars<br />$ cat my-variables.tfvars<br />image-id = "YOUR-ID3"<br />$ terraform init<br />$ terraform plan -var-file=my-variables.tfvars<br />$ terraform apply -var-file=my-variables.tfvars<br />$ terraform state list<br />$ vim my-config.tf<br />$ cat my-config.tf<br />terraform {<br />&nbsp; required_providers {<br />&nbsp;&nbsp;&nbsp; yandex = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; source = "yandex-cloud/yandex"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />}<br />provider "yandex" {<br />&nbsp; token&nbsp; =&nbsp; "YOUR-TOKEN"<br />&nbsp; cloud_id&nbsp; = "YOUR-ID1"<br />&nbsp; folder_id = "YOUR-ID2"<br />&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />}<br />variable "image-id" {<br />&nbsp; type = string<br />}<br />resource "yandex_compute_instance" "vm-1" {<br />&nbsp; name = "from-terraform-vm"<br />&nbsp; platform_id = "standard-v1"<br />&nbsp; zone = "ru-central1-a"<br />&nbsp; resources {<br />&nbsp;&nbsp;&nbsp; cores&nbsp; = 2<br />&nbsp;&nbsp;&nbsp; memory = 2<br />&nbsp; }<br />&nbsp; boot_disk {<br />&nbsp;&nbsp;&nbsp; initialize_params {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id = var.image-id<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; }<br />&nbsp; network_interface {<br />&nbsp;&nbsp;&nbsp; subnet_id = yandex_vpc_subnet.subnet-1.id<br />&nbsp;&nbsp;&nbsp; nat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = true<br />&nbsp; }<br />&nbsp; metadata = {<br />&nbsp;&nbsp;&nbsp; ssh-keys = "ubuntu:${file("/YOUR-DIR/YOUR-KEY.pub")}"<br />&nbsp; }<br />}<br />resource "yandex_vpc_network" "network-1" {<br />&nbsp; name = "from-terraform-network"<br />}<br />resource "yandex_vpc_subnet" "subnet-1" {<br />&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "from-terraform-subnet"<br />&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp; network_id&nbsp;&nbsp;&nbsp;&nbsp; = yandex_vpc_network.network-1.id<br />&nbsp; v4_cidr_blocks = ["10.2.0.0/16"]<br />}<br />resource "yandex_mdb_postgresql_cluster" "YOUR-DB" {<br />&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "YOUR-DB"<br />&nbsp; environment = "PRESTABLE"<br />&nbsp; network_id&nbsp; = yandex_vpc_network.network-1.id<br />&nbsp; config {<br />&nbsp;&nbsp;&nbsp; version = 12<br />&nbsp;&nbsp;&nbsp; resources {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; resource_preset_id = "s2.micro"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_type_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "network-ssd"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 16<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; postgresql_config = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; max_connections&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 395<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; enable_parallel_hash&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vacuum_cleanup_index_scale_factor = 0.2<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; autovacuum_vacuum_scale_factor&nbsp;&nbsp;&nbsp; = 0.34<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; default_transaction_isolation&nbsp;&nbsp;&nbsp;&nbsp; = "TRANSACTION_ISOLATION_READ_COMMITTED"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; shared_preload_libraries&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; } <br />&nbsp; database {<br />&nbsp;&nbsp;&nbsp; name&nbsp; = "YOUR-DB"<br />&nbsp;&nbsp;&nbsp; owner = "YOUR-USERNAME"<br />&nbsp; } <br />&nbsp; user {<br />&nbsp;&nbsp;&nbsp; name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "YOUR-USERNAME"<br />&nbsp;&nbsp;&nbsp; password&nbsp;&nbsp; = "YOUR-PASSWORD"<br />&nbsp;&nbsp;&nbsp; conn_limit = 50<br />&nbsp;&nbsp;&nbsp; permission {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; database_name = "YOUR-DB"<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp; settings = {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; default_transaction_isolation = "read committed"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log_min_duration_statement&nbsp;&nbsp;&nbsp; = 5000<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; } <br />&nbsp; host {<br />&nbsp;&nbsp;&nbsp; zone&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = "ru-central1-a"<br />&nbsp;&nbsp;&nbsp; subnet_id = yandex_vpc_subnet.subnet-1.id<br />&nbsp; }<br />} <br />output "internal_ip_address_vm_1" {<br />&nbsp; value = yandex_compute_instance.vm-1.network_interface.0.ip_address<br />} <br />output "external_ip_address_vm_1" {<br />&nbsp; value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address<br />}<br />$ terraform plan -var-file=my-variables.tfvars<br />$ terraform apply -var-file=my-variables.tfvars<br />$ terraform state list<br />$ terraform destroy -var-file=my-variables.tfvars<br /><strong>Task:</strong><br />Создание докер-образа и загрузка его в Container Registry<br />В этой практической работе вы создадите реестр в Yandex Container Registry, подготовите Docker-образ виртуальной машины и поместите его в реестр, <br />а затем создадите машину из этого образа.<br /><strong>Decision:</strong><br />Установите Docker. Создайте реестр в Yandex Container Registry:<br />&nbsp;yc container registry create --name my-registry<br />Обратите внимание, что в выводе есть уникальный идентификатор (id) реестра. Он пригодится вам для следующих команд.<br />&nbsp;id: crpfpd8jhhldiqah91rc<br />&nbsp;folder_id: b1gfdbij3ijgopgqv9m9<br />&nbsp;name: my-registry<br />&nbsp;status: ACTIVE<br />&nbsp;created_at: "2021-04-06T00:46:48.150Z"<br />Аутентифицируйтесь в Yandex Container Registry с помощью Docker Credential helper. Это нужно для того, чтобы внешняя платформа Docker могла от вашего имени отправить образ в ваш приватный реестр в Yandex Cloud.<br />&nbsp;yc container registry configure-docker<br />Подготовьте Dockerfile. Можно использовать файл из урока о Docker:<br />&nbsp;FROM ubuntu:latest<br />&nbsp;RUN apt-get update -y<br />&nbsp;RUN apt-get install -y nginx<br />&nbsp;ENTRYPOINT ["nginx", "-g", "daemon off;"]<br />По умолчанию Docker использует файл с именем Dockerfile и без расширения.<br />Перейдите в папку с Dockerfile и соберите образ (не забудьте подставить идентификатор своего реестра):<br />&nbsp;docker build . -t cr.yandex/&lt;идентификатор_реестра&gt;/ubuntu-nginx:latest<br />Ключ -t позволяет задать образу имя.<br />Напоминаем, что в Yandex Container Registry можно загрузить только образы, названные по такому шаблону:<br />&nbsp;cr.yandex/&lt;ID реестра&gt;/&lt;имя Docker-образа&gt;:&lt;тег&gt;<br />Загрузите Docker-образ в реестр:<br />&nbsp;docker push cr.yandex/&lt;идентификатор_реестра&gt;/ubuntu-nginx:latest<br />В консоли управления перейдите в реестр и предоставьте всем пользователям право использовать хранящиеся образы. Для этого перейдите на вкладку Права доступа, в правом верхнем углу нажмите кнопку Назначить роли. В открывшемся окне нажмите кнопку Выбрать пользователя, на вкладке Группы выберите All users. Нажмите кнопку Добавить роль и последовательно введите viewer и container-registry.images.puller. Нажмите кнопку Сохранить.<br />В консоли управления создайте ВМ с помощью Container Optimized Image.<br />При создании машины в разделе Выбор образа загрузочного диска переключитесь на вкладку Container Solution и нажмите Настроить. Выберите из реестра созданный образ, остальные настройки оставьте по умолчанию и нажмите Применить.<br />Другие настройки ВМ мы уже разбирали.<br />Когда новая ВМ получит статус Running, найдите её внешний IP адрес в консоли управления и убедитесь, что по этому адресу отображается приветственная страница NGINX.<br />Обратите внимание! C помощью Docker-образа вы создали и запустили виртуальную машину с предустановленным, нужным вам ПО. При этом вам даже не потребовалось заходить внутрь ВМ и выполнять установку или настройку ПО вручную.<br /><strong>Decision:</strong><br />$ sudo apt update<br />$ sudo apt install apt-transport-https ca-certificates curl software-properties-common<br />$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -<br />$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"<br />$ sudo apt update<br />$ apt-cache policy docker-ce<br />$ sudo apt install docker-ce<br />$ sudo /etc/init.d/docker start<br />$ yc container registry create --name my-registry<br />$ yc container registry configure-docker<br />$ cd docker/<br />$ vim Dockerfile<br />$ cat Dockerfile<br />FROM ubuntu:latest<br />RUN apt-get update -y<br />RUN apt-get install -y nginx<br />ENTRYPOINT ["nginx", "-g", "daemon off;"]<br />$ sudo docker build . -t cr.yandex/YOUR-ID/ubuntu-nginx:latest<br />$ sudo docker push cr.yandex/YOUR-ID/ubuntu-nginx:latest<br /><strong>Task:</strong><br />Создание кластера. В этой практической работе вы создадите кластер Kubernetes и группу узлов в нём.<br /><strong>Decision:</strong><br />Выберите каталог для кластера.<br />Выберите сервис Managed Service for Kubernetes. Нажмите кнопку Создать кластер. Дальше заполним настройки кластера:<br />Для Kubernetes необходим сервисный аккаунт для ресурсов и узлов.<br />Сервисный аккаунт для ресурсов &mdash; это аккаунт, под которым сервису Kubernetes будут выделяться ресурсы в нашем облаке.<br />Сервисный аккаунт для узлов необходим уже созданным узлам самого кластера Kubernetes для доступа к другим ресурсам. Например, чтобы получить Docker-образы из Container Registry.<br />Этим аккаунтам нужны разные права, и поэтому у них бывают разные роли. В общем случае вы можете использовать один и тот же сервисный аккаунт. Выберите аккаунт, который создали на первом курсе, или заведите новый.<br />Ключ шифрования Yandex Key Management Service позволяет защитить конфиденциальную информацию (пароли, OAuth-токены и SSH-ключи) и повысить безопасность. Это необязательно &mdash; кластер запустится и без ключа. Для этой практической работы не создавайте его.<br />Релизные каналы RAPID, REGULAR и STABLE отличаются процессом обновления и доступными вам версиями Kubernetes.<br />RAPID и REGULAR содержат все версии, включая минорные. STABLE &mdash; только стабильные версии. RAPID обновляется автоматически, а в REGULAR и STABLE обновление можно отключить. Когда появляется обновление, информация о нём отображается в консоли управления.<br />Выберите REGULAR.<br />Внимательно выбирайте релизный канал! Изменить его после создания кластера Kubernetes нельзя.<br />Конфигурация мастера. Мастер &mdash; ведущая нода группы узлов кластера &mdash; следит за состоянием Kubernetes и запускает управляющие процессы. Сконфигурируем мастер:<br />Выберите версию Kubernetes. Их набор зависит от релизного канала. Версии мастера и других нод могут не совпадать, но это достаточно тонкая настройка, могут возникнуть проблемы совместимости, которые повлияют на работу всего кластера.<br />Кластеру может назначаться публичный IP-адрес. Выберите вариант Автоматически. В этом случае IP выбирается из пула свободных IP-адресов. Если вы не используете Cloud Interconnect или VPN для подключения к облаку, то без автоматического назначения IP-адресов вы не сможете подключиться к кластеру: он будет доступен только во внутренней сети вашего облака.<br />Тип мастера влияет на отказоустойчивость. Зональный работает только в одной зоне доступности, а региональный &mdash; в трёх подсетях в каждой зоне доступности.<br />Выберите зональный тип. В будущем для рабочей среды используйте региональные кластеры, а для разработки и тестирования &mdash; более дешёвые зональные.<br />Выбор типа мастера также влияет на подсети, в которых будет развёрнут кластер. У вас уже есть подсети, созданные по умолчанию для функционирования облака. Выберите их.<br />Настройки окна обновлений.<br />Режимов обновления четыре: Отключено, В любое время, Ежедневно и В выбранные дни. Региональный мастер во время обновления остаётся доступен, зональный &mdash; нет.<br />Группа узлов кластера обновляется с выделением дополнительных ресурсов, так как при обновлении создаются узлы с обновлённой конфигурацией. При обновлении поды с контейнерами будут переезжать с одного узла на другой.<br />По умолчанию выставлен пункт В любое время. Оставьте его.<br />Сетевые настройки кластера. Сетевые политики для кластера Kubernetes необязательны. Эта опция включает сетевой контроллер Calico, который позволяет применять тонкие настройки политик доступа для кластера. Не выбирайте эту опцию.<br />Во время работы кластера подам с контейнерами и сервисам самого кластера Kubernetes будут автоматически присваиваться внутренние IP-адреса. Чтобы IP-адреса подов и сервисов Kubernetes не пересеклись с другими адресами в вашем облаке, задайте CIDR (Classless Inter-Domain Routing &mdash; бесклассовая междоменная маршрутизация). Оставьте адреса пустыми: они будут назначены автоматически.<br />Маска подсети узлов влияет на количество подов, которые могут запускаться. Если адресов не хватит, под не запустится.<br />Вы заполнили все настройки, теперь нажмите Создать кластер. Дождитесь, пока статус кластера станет RUNNING, а состояние &mdash; HEALTHY. Это может занять около 10 минут.<br />Создание группы узлов. Зайдите в созданный кластер, перейдите на вкладку Управление узлами и нажмите Создать группу узлов. Группы узлов &mdash; это группы виртуальных машин.<br />Введите имя и описание группы, выберите версию Kubernetes. Выберите Автоматический тип масштабирования и количество узлов от 1 до 5. Укажите среду запуска контейнеров &mdash; Docker.<br />В сетевых настройках задайте автоматический IP-адрес и выберите зону доступности (кластер зональный, поэтому зона доступности только одна). Задайте SSH-ключ, чтобы иметь доступ к виртуальным машинам кластера. Настройки обновления идентичны настройкам мастера.<br />Остальные настройки группы, которые мы не упомянули (вычислительные ресурсы, хранилище и т. д.), оставьте по умолчанию.<br />Нажмите Создать группу узлов и дождитесь, пока операция выполнится.<br /><strong>Task:</strong><br />Первое приложение в кластере. <br />На прошлом уроке вы создали в консоли управления Yandex Cloud кластер Kubernetes и группу узлов в нём. <br />Теперь с помощью командной строки вы развернете в кластере приложение &mdash; веб-сервер NGINX.<br /><strong>Decision:</strong><br />В консоли управления войдите в созданный кластер Managed Service for Kubernetes и нажмите кнопку Подключиться. В открывшемся окне скопируйте команду для подключения:<br />&nbsp;yc managed-kubernetes cluster get-credentials \<br />&nbsp;&nbsp; --id &lt;идентификатор_кластера&gt; \<br />&nbsp;&nbsp; --external<br />Чтобы проверить правильность установки и подключения, посмотрите на конфигурацию:<br />&nbsp;kubectl config view<br />Ответ получится примерно таким (IP-адрес сервера и название кластера будут отличаться):<br />&nbsp;apiVersion: v1<br />&nbsp;clusters:<br />&nbsp;- cluster:<br />&nbsp;&nbsp;&nbsp;&nbsp; certificate-authority-data: DATA+OMITTED<br />&nbsp;&nbsp;&nbsp;&nbsp; server: https://178.154.206.242<br />&nbsp;&nbsp; name: yc-managed-k8s-cat2oek6hbp7mnhhhr4m<br />&nbsp;contexts:<br />&nbsp;...<br />Создание манифеста<br />Для описания настроек приложения в кластере создадим файл my-nginx.yaml. Такой файл называется манифестом.<br />apiVersion: apps/v1<br />kind: Deployment<br />metadata:<br />&nbsp; name: my-nginx-deployment<br />spec:<br />&nbsp; replicas: 1<br />&nbsp; selector:<br />&nbsp;&nbsp;&nbsp; matchLabels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx<br />&nbsp; template:<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; labels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx<br />&nbsp;&nbsp;&nbsp; spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; containers:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: nginx<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image: cr.yandex/&lt;идентификатор_реестра&gt;/ubuntu-nginx:latest <br />Рассмотрим, из чего он состоит.<br />Директива apiVersion определяет, для какой версии Kubernetes написан манифест. От версии к версии обозначение может меняться.<br />&nbsp;apiVersion: apps/v1<br />Директива kind описывает механизм использования. Она может принимать значения Deployment, Namespace, Service, Pod, LoadBalancer и т. д. Для развёртывания приложения укажите значение Deployment.<br />&nbsp;kind: Deployment<br />Директива metadata определяет метаданные приложения: имя, метки (labels), аннотации.<br />С помощью Меток можно идентифицировать, группировать объекты, выбирать их подмножества. Добавляйте и изменяйте метки при создании объектов или позднее, в любое время.<br />Аннотации используют, чтобы добавить собственные метаданные к объектам.<br />Укажем имя приложения:<br />&nbsp;metadata:<br />&nbsp;&nbsp; name: my-nginx-deployment<br />В основном блоке spec содержится описание объектов Kubernetes.<br />Директива replicas определяет масштабирование. Для первого запуска укажите, что приложению нужен один под. Позже вы посмотрите, как приложения масштабируются, и сможете увеличить число подов.<br />Директива selector определяет, какими подами будет управлять контейнер (подробнее о ней можно прочитать в документации). Поды отбираются с помощью метки (label).<br />Директива template определяет шаблон пода. Метка в шаблоне должна совпадать с меткой селектора &mdash; nginx.<br />В шаблоне содержится ещё одна, собственная директива spec. Она задаёт настройки контейнеров, которые будет развёрнуты на поде. Нам нужен один контейнер. Используйте для него образ, созданный ранее с помощью Docker и помещённый в реестр Yandex Container Registry.<br />&nbsp;spec: <br />&nbsp;&nbsp; matchLabels: <br />&nbsp;&nbsp;&nbsp;&nbsp; app: nginx<br />&nbsp;&nbsp; replicas: 1<br />&nbsp;&nbsp; selector: ~<br />&nbsp;&nbsp; template: <br />&nbsp;&nbsp;&nbsp;&nbsp; metadata: <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; labels: <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx<br />&nbsp;&nbsp;&nbsp;&nbsp; spec: <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; containers: <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: nginx<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image: "cr.yandex/&lt;идентификатор_реестра&gt;/ubuntu-nginx:latest"<br />Выполнение манифеста. Для создания или обновления ресурсов в кластере используется команда apply. Файл манифеста указывается после флага -f.<br />&nbsp;kubectl apply -f &lt;путь_к_файлу_my-nginx.yaml&gt;<br />Если результат будет успешным, вы увидите сообщение:<br />&nbsp;deployment.apps/my-nginx-deployment created<br />Чтобы убедиться, что приложение создано, посмотрите список подов:<br />&nbsp;kubectl get pods<br />Дождитесь статуса Running:<br />&nbsp;NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; READY&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp; RESTARTS&nbsp;&nbsp; AGE<br />&nbsp;my-nginx-deployment-65b9b678b6-zmfww&nbsp;&nbsp; 1/1&nbsp;&nbsp;&nbsp;&nbsp; Running&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5m27s<br />Теперь получите более подробную информацию, выполнив ту же команду с флагом -o wide:<br />&nbsp;kubectl get pods -o wide<br />Вы увидите внутренний IP-адрес, который присвоен поду. Это пригодится, если нужно узнать, где именно развёрнуто приложение.<br />Чтобы получить максимально подробную информацию о запущенном приложении, используйте команду describe:<br />&nbsp;kubectl describe deployment/my-nginx-deployment<br />Масштабирование. Теперь увеличьте количество подов. Вручную это можно сделать двумя способами:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; изменить файл манифеста, указав в директиве replicas нужное число подов, и снова выполнить команду apply;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; если файла манифеста нет под рукой &mdash; использовать команду scale:<br />kubectl scale --replicas=3 deployment/my-nginx-deployment <br />Если всё получится, в выводе команды kubectl get pods вы увидите сообщение:<br />NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; READY&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp; RESTARTS&nbsp;&nbsp; AGE<br />my-nginx-deployment-65b9b678b6-6whpp&nbsp;&nbsp; 1/1&nbsp;&nbsp;&nbsp;&nbsp; Running&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 117s<br />my-nginx-deployment-65b9b678b6-wtph9&nbsp;&nbsp; 1/1&nbsp;&nbsp;&nbsp;&nbsp; Running&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 117s<br />my-nginx-deployment-65b9b678b6-zmfww&nbsp;&nbsp; 1/1&nbsp;&nbsp;&nbsp;&nbsp; Running&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14m <br />На следующей практической работе мы посмотрим, как обращаться извне к кластеру Kubernetes и развёрнутому в нём приложению.<br />Кластер как код. Как видите, управление кластерами Kubernetes отлично вписывается в концепцию Infrastructure as Code: вы можете описать конфигурацию кластера в текстовом файле &mdash; манифесте. Вы также можете разворачивать кластеры Kubernetes с помощью Terraform.<br /><strong>Decision:</strong><br />$ sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https<br />$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -<br />$ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list<br />$ sudo apt-get update<br />$ sudo apt-get install -y kubectl<br />$ yc managed-kubernetes cluster get-credentials \<br />&nbsp;&nbsp; --id YOUR-ID1 \<br />&nbsp;&nbsp; --external<br />$ kubectl config view<br />$ vim my-nginx.yaml<br />$ cat my-nginx.yaml<br />apiVersion: apps/v1<br />kind: Deployment<br />metadata:<br />&nbsp; name: my-nginx-deployment<br />spec:<br />&nbsp; replicas: 1<br />&nbsp; selector:<br />&nbsp;&nbsp;&nbsp; matchLabels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx<br />&nbsp; template:<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; labels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx<br />&nbsp;&nbsp;&nbsp; spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; containers:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: nginx<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image: cr.yandex/YOUR-ID/ubuntu-nginx:latest<br />$ kubectl apply -f my-nginx.yaml<br />$ kubectl get pods<br />$ kubectl get pods -o wide<br />$ kubectl describe deployment/my-nginx-deployment<br />$ kubectl scale --replicas=3 deployment/my-nginx-deployment<br />$ kubectl get pods<br /><strong>Task:</strong><br />Балансировка нагрузки.<br />Большинство веб-приложений созданы, чтобы взаимодействовать через интернет. <br />Вы развернули в кластере приложение, но у вас пока нет к нему доступа из интернета. Чтобы исправить эту проблему, воспользуемся сервисом LoadBalancer.<br />У созданного пода есть внутренний IP-адрес.<br />Помните, мы говорили о том, что в кластере есть собственный сервис DNS? Он работает с внутренними IP-адресами объектов кластера, чтобы те могли взаимодействовать.<br />Однако внутренний IP-адрес может меняться, когда ресурсы группы узлов обновляются. Чтобы обращаться к приложению извне, требуется неизменный публичный IP-адрес &mdash; это и будет IP-адрес балансировщика.<br /><strong>Decision:</strong><br />Создайте файл-манифест load-balancer.yaml:<br />&nbsp;apiVersion: v1<br />&nbsp;kind: Service<br />&nbsp;metadata:<br />&nbsp;&nbsp; name: my-loadbalancer<br />&nbsp;spec:<br />&nbsp;&nbsp; selector:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx<br />&nbsp;&nbsp; ports:<br />&nbsp;&nbsp; - port: 80<br />&nbsp;&nbsp;&nbsp;&nbsp; targetPort: 80<br />&nbsp;&nbsp; type: LoadBalancer<br />Где: port &mdash; порт сетевого балансировщика, на котором будут обслуживаться пользовательские запросы; targetPort &mdash; порт контейнера, на котором доступно приложение; selector &mdash; метка селектора из шаблона подов в манифесте объекта Deployment.<br />Выполните манифест:<br />&nbsp;kubectl apply -f &lt;путь_к_файлу_load-balancer.yaml&gt;<br />Вы увидите сообщение:<br />&nbsp;service/my-loadbalancer created<br />В консоли управления откройте раздел Load Balancer. Там должен появиться балансировщик нагрузки с префиксом k8s в имени и уникальным идентификатором кластера Kubernetes.<br />Скопируйте IP-адрес балансировщика в адресную строку браузера. Вы увидите приветственную страницу NGINX.<br />Если при создании ресурсов вы получаете ошибку failed to ensure cloud loadbalancer: failed to start cloud lb creation: Permission denied, убедитесь, что вашему сервисному аккаунту хватает прав. Подробнее читайте в документации. <br /><strong>Decision:</strong><br />$ vim load-balancer.yaml<br />$ cat load-balancer.yaml<br />&nbsp;apiVersion: v1<br />&nbsp;kind: Service<br />&nbsp;metadata:<br />&nbsp;&nbsp; name: my-loadbalancer<br />&nbsp;spec:<br />&nbsp;&nbsp; selector:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx<br />&nbsp;&nbsp; ports:<br />&nbsp;&nbsp; - port: 80<br />&nbsp;&nbsp;&nbsp;&nbsp; targetPort: 80<br />&nbsp;&nbsp; type: LoadBalancer<br />$ kubectl apply -f load-balancer.yaml<br /><strong>Task:</strong><br />Автомасштабирование в Yandex Managed Kubernetes. <br />В этой работе вы увидите, как в Kubernetes выполняется горизонтальное автомасштабирование.<br /><strong>Decision:</strong><br />Создайте манифест load-balancer-hpa.yaml.<br />Для начала скопируйте в него настройки спецификаций, которые вы составляли на предыдущих уроках: из my-nginx.yaml (в примере ниже это раздел Deployment) и из load-balancer.yaml (раздел Service).<br />Поскольку новый балансировщик должен отслеживать отдельную группу контейнеров, используйте для контейнеров другие метки (labels), например nginx-hpa.<br />---<br />### Deployment<br />apiVersion: apps/v1<br />kind: Deployment<br />metadata:<br />&nbsp; name: my-loadbalancer-hpa<br />&nbsp; labels:<br />&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />spec:<br />&nbsp; replicas: 1<br />&nbsp; selector:<br />&nbsp;&nbsp;&nbsp; matchLabels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp; template:<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: nginx-hpa<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; labels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp;&nbsp;&nbsp; spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; containers:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: nginx-hpa<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image: k8s.gcr.io/hpa-example<br />---<br />### Service<br />apiVersion: v1<br />kind: Service<br />metadata:<br />&nbsp; name: my-loadbalancer-hpa<br />spec:<br />&nbsp; selector:<br />&nbsp;&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp; ports:<br />&nbsp;&nbsp;&nbsp; - protocol: TCP<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; port: 80<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; targetPort: 80<br />&nbsp; type: LoadBalancer <br />В разделе Deployment смените образ с Yandex Container Registry на k8s.gcr.io/hpa-example &mdash; это специальный тестовый образ из публичного репозитория, создающий высокую нагрузку на процессор. Так вам будет удобно отслеживать работу Horizontal Pod Autoscaler.<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; containers:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: nginx-hpa<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image: k8s.gcr.io/hpa-example <br />Теперь добавьте в шаблон контейнера настройки requests и limits: мы попросим по умолчанию 256 мебибайтов памяти и 500 милли-CPU (половину ядра), а ограничим контейнер 500 мебибайтами и 1 CPU.<br />&nbsp;&nbsp;&nbsp; ...<br />&nbsp;&nbsp;&nbsp; spec:<br />&nbsp;&nbsp;&nbsp;&nbsp; containers:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: nginx-hpa<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image: k8s.gcr.io/hpa-example<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; resources:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; requests:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: "256Mi"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu: "500m"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; limits:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: "500Mi"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu: "1" <br />Дополните манифест настройками для Horizontal Pod Autoscaler:<br />apiVersion: autoscaling/v1<br />kind: HorizontalPodAutoscaler<br />metadata:<br />&nbsp; name: my-hpa<br />spec:<br />&nbsp; scaleTargetRef:<br />&nbsp;&nbsp;&nbsp; apiVersion: apps/v1<br />&nbsp;&nbsp;&nbsp; kind: Deployment<br />&nbsp;&nbsp;&nbsp; name: my-nginx-deployment-hpa<br />&nbsp; minReplicas: 1<br />&nbsp; maxReplicas: 5<br />&nbsp; targetCPUUtilizationPercentage: 20 <br />В результате должен получиться такой манифест:<br />---<br />### Deployment<br />apiVersion: apps/v1<br />kind: Deployment<br />metadata:<br />&nbsp; name: my-nginx-deployment-hpa<br />&nbsp; labels:<br />&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />spec:<br />&nbsp; replicas: 1<br />&nbsp; selector:<br />&nbsp;&nbsp;&nbsp; matchLabels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp; template:<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: nginx-hpa<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; labels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp;&nbsp;&nbsp; spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; containers:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: nginx-hpa<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image: k8s.gcr.io/hpa-example<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; resources:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; requests:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: "256Mi"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu: "500m"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; limits:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: "500Mi"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu: "1"<br />---<br />### Service<br />apiVersion: v1<br />kind: Service<br />metadata:<br />&nbsp; name: my-loadbalancer-hpa<br />spec:<br />&nbsp; selector:<br />&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp; ports:<br />&nbsp;&nbsp;&nbsp; - protocol: TCP<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; port: 80<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; targetPort: 80<br />&nbsp; type: LoadBalancer<br />---<br />### HPA<br />apiVersion: autoscaling/v1<br />kind: HorizontalPodAutoscaler<br />metadata:<br />&nbsp; name: my-hpa<br />spec:<br />&nbsp; scaleTargetRef:<br />&nbsp;&nbsp;&nbsp; apiVersion: apps/v1<br />&nbsp;&nbsp;&nbsp; kind: Deployment<br />&nbsp;&nbsp;&nbsp; name: my-nginx-deployment-hpa<br />&nbsp; minReplicas: 1<br />&nbsp; maxReplicas: 5<br />&nbsp; targetCPUUtilizationPercentage: 20 <br />Примените манифест:<br />kubectl apply -f &lt;путь_к_load-balancer-hpa.yaml&gt; <br />Вы увидите три сообщения:<br />deployment.apps/my-nginx-deployment-hpa created<br />service/my-loadbalancer-hpa created<br />horizontalpodautoscaler.autoscaling/my-hpa created <br />В консоли управления перейдите в раздел Network Load Balancer. Дождитесь, пока статус my-nginx-deployment-hpa станет Running, после чего посмотрите IP-адрес балансировщика. Убедитесь, что в браузере этот адрес доступен. В терминале сохраните IP-адрес в переменную. Например, так:<br />LOAD_BALANCER_IP=&lt;IP-адрес балансировщика&gt; <br />Запустите в отдельном окне отслеживание интересующих вас компонентов кластера Kubernetes:<br />while true; do kubectl get pod,svc,hpa,nodes -o wide; sleep 5; done &nbsp;<br />Теперь сымитируйте рабочую нагрузку на приложение. Для этого подойдёт утилита wget (установите её с помощью пакетного менеджера или с сайта).<br />while true; do wget -q -O- http://$LOAD_BALANCER_IP; done &nbsp;<br />Вы увидите, что сначала увеличится число подов, а затем добавятся узлы. Число узлов ограничено настройками группы узлов кластера, которые вы задали при создании кластера (в нашем случае максимальное количество узлов &mdash; пять).<br />Остановите цикл создания нагрузки на приложение (комбинация клавиш Ctrl + C). В окне консоли с отслеживанием компонентов кластера вы увидите, как удаляются узлы и поды без нагрузки.<br /><strong>Decision:</strong><br />$ vim load-balancer-hpa.yaml<br />$ cat load-balancer-hpa.yaml<br />### Deployment<br />apiVersion: apps/v1<br />kind: Deployment<br />metadata:<br />&nbsp; name: my-nginx-deployment-hpa<br />&nbsp; labels:<br />&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />spec:<br />&nbsp; replicas: 1<br />&nbsp; selector:<br />&nbsp;&nbsp;&nbsp; matchLabels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp; template:<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: nginx-hpa<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; labels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp;&nbsp;&nbsp; spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; containers:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: nginx-hpa<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image: k8s.gcr.io/hpa-example<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; resources:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; requests:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: "256Mi"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu: "500m"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; limits:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: "500Mi"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu: "1"<br />### Service<br />apiVersion: v1<br />kind: Service<br />metadata:<br />&nbsp; name: my-loadbalancer-hpa<br />spec:<br />&nbsp; selector:<br />&nbsp;&nbsp;&nbsp; app: nginx-hpa<br />&nbsp; ports:<br />&nbsp;&nbsp;&nbsp; - protocol: TCP<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; port: 80<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; targetPort: 80<br />&nbsp; type: LoadBalancer<br />### HPA<br />apiVersion: autoscaling/v1<br />kind: HorizontalPodAutoscaler<br />metadata:<br />&nbsp; name: my-hpa<br />spec:<br />&nbsp; scaleTargetRef:<br />&nbsp;&nbsp;&nbsp; apiVersion: apps/v1<br />&nbsp;&nbsp;&nbsp; kind: Deployment<br />&nbsp;&nbsp;&nbsp; name: my-nginx-deployment-hpa<br />&nbsp; minReplicas: 1<br />&nbsp; maxReplicas: 5<br />&nbsp; targetCPUUtilizationPercentage: 20<br />$ kubectl apply -f load-balancer-hpa.yaml<br /><strong>Task:</strong><br />Сбой виртуальной машины.<br />Давайте посмотрим, как принципы построения отказоустойчивых систем реализованы в Yandex Cloud. <br />В практических работах этой темы вы проверите четыре основных сценария отказов: сбой виртуальной машины, сбой всей зоны доступности, обновление приложения, сбой приложения.<br />Вы сымитируете эти отказы и понаблюдаете, как Yandex Cloud обеспечивает доступность приложения и восстанавливает инфраструктуру после сбоев.<br /><strong>Decision:</strong><br />Начнем с самого простого сценария &mdash; сбоя виртуальной машины.<br />Создайте группу из трёх ВМ в трёх зонах доступности под балансировщиком нагрузки. Используйте образ с ОС Ubuntu 18.04 (потом мы обновим его на более свежую версию ОС).<br />Используйте спецификацию specification.yaml из практической работы по CLI Yandex Cloud, но адаптируйте её для того, чтобы на ней можно было проверить разные сценарии сбоев.<br />Во-первых, будут задействованы все три зоны доступности, поэтому нужно немного исправить блок allocation_policy:<br />allocation_policy:<br />&nbsp;&nbsp;&nbsp; zones:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-a<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-b<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-c <br />Также пропишите подсети для каждой зоны (не забывайте подставлять идентификаторы ваших подсетей):<br />&nbsp;&nbsp;&nbsp; network_interface_specs:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - network_id: &lt;идентификатор_сети&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subnet_ids: <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - &lt;идентификатор_подсети_№1&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - &lt;идентификатор_подсети_№2&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - &lt;идентификатор_подсети_№3&gt; <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }} <br />Во-вторых, в секции #cloud-config укажите пользователя, которого нужно создать для входа в виртуальные машины по SSH (это понадобится позднее, на одной из следующих практических работ):<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; users:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: my-user<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; groups: sudo<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lock_passwd: true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sudo: 'ALL=(ALL) NOPASSWD:ALL'<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ssh-authorized-keys:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - ssh-rsa AAAAB3Nza... <br />Обновленный файл спецификации specification.yaml:<br />name: my-group<br />service_account_id: &lt;идентификатор_сервисного_аккаунта&gt;<br />instance_template:<br />&nbsp;&nbsp;&nbsp; platform_id: standard-v1<br />&nbsp;&nbsp;&nbsp; resources_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: 2g<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cores: 2<br />&nbsp;&nbsp;&nbsp; boot_disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mode: READ_WRITE<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id: &lt;идентификатор_образа_Ubuntu_18.04&gt; <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; type_id: network-hdd<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 32g<br />&nbsp;&nbsp;&nbsp; network_interface_specs:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - network_id: &lt;идентификатор_сети&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subnet_ids: <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - &lt;идентификатор_подсети_№1&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - &lt;идентификатор_подсети_№2&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - &lt;идентификатор_подсети_№3&gt; <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}<br />&nbsp;&nbsp;&nbsp; scheduling_policy:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; preemptible: false<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; user-data: |-<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #cloud-config<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; users:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: my-user<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; groups: sudo<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lock_passwd: true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sudo: 'ALL=(ALL) NOPASSWD:ALL'<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ssh-authorized-keys:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - &lt;содержимое_публичной_части_SSH-ключа&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; package_update: true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; runcmd:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [ apt-get, install, -y, nginx ]<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']<br />&nbsp;deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 1<br />&nbsp;&nbsp;&nbsp; max_expansion: 0<br />scale_policy:<br />&nbsp;&nbsp;&nbsp; fixed_scale:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 3<br />allocation_policy:<br />&nbsp;&nbsp;&nbsp; zones:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-a<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-b<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-c<br />&nbsp;load_balancer_spec:<br />&nbsp;&nbsp;&nbsp; target_group_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: my-target-group<br />Создайте группу по новой спецификации:<br />yc compute instance-group create --file &lt;путь_к_файлу_specification.yaml&gt; <br />Если ранее вы удаляли балансировщик нагрузки, создайте его снова и привяжите к целевой группе:<br />yc load-balancer network-load-balancer create \<br />&nbsp; --region-id ru-central1 \<br />&nbsp; --name my-load-balancer \<br />&nbsp; --listener name=my-listener,external-ip-version=ipv4,port=80 \<br />&nbsp; --target-group target-group-id=&lt;идентификатор_целевой_группы&gt;,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80 <br />В консоли управления убедитесь, что ресурсы созданы. Проверьте вывод по внешнему IP-адресу балансировщика &mdash; должна отображаться приветственная страница с идентификатором одной из виртуальных машин группы.<br />Начните отслеживать состояние виртуальных машин группы и целевой группы балансировщика:<br />while true; do \<br />yc compute instance-group \<br />&nbsp; --id &lt;идентификатор_группы_ВМ&gt; list-instances; \<br />yc load-balancer network-load-balancer \<br />&nbsp; --id &lt;идентификатор_балансировщика&gt; target-states \<br />&nbsp; --target-group-id &lt;идентификатор_целевой_группы&gt;; \<br />sleep 5; done <br />Информация выводится в виде таблиц:<br />+----------------------+---------------------------+----------------+-------------+------------------------+----------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | STATUS MESSAGE |<br />+----------------------+---------------------------+----------------+-------------+------------------------+----------------+<br />| ef34nv4tp3ha8gl6p3df | cl1m5ksvljnq5frekghi-uzex | 84.201.148.207 | 10.128.0.42 | RUNNING_ACTUAL [1m54s] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [13m]&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [6h]&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+------------------------+----------------+<br />+----------------------+-------------+---------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; | STATUS&nbsp; |<br />+----------------------+-------------+---------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |<br />| e2luooifg8ruecr7g6fk | 10.128.0.6&nbsp; | HEALTHY |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY |<br />+----------------------+-------------+---------+<br />Сбой виртуальной машины может произойти из-за падения физического хоста, на котором она запущена. Иногда виртуальную машину могут удалить случайно, по ошибке. Чтобы сымитировать сбой, удалим одну из виртуальных машин в группе через консоль управления.<br />Если бы это была единственная машина, на которую поступает трафик, система стала бы недоступна. Но у нас система развернута на нескольких виртуальных машинах, поэтому трафик будет перенаправлен на две оставшиеся. Через несколько секунд будет обнаружена проблема, и виртуальная машина будет выведена из-под балансировки. Об этом говорит статус UNHEALTHY.<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | STATUS MESSAGE |<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />| ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33&nbsp; | 10.128.0.6&nbsp; | RUNNING_ACTUAL [15m] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [32m] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [6h]&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />+----------------------+-------------+-----------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; |&nbsp; STATUS&nbsp;&nbsp; |<br />+----------------------+-------------+-----------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY&nbsp;&nbsp; |<br />| e2luooifg8ruecr7g6fk | 10.128.0.6&nbsp; | UNHEALTHY |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY&nbsp;&nbsp; |<br />+----------------------+-------------+-----------+ <br />Далее подсеть перейдет в статус DRAINING &mdash; ресурс удаляется, и с него снимается трафик. Балансировщик перестает передавать трафик этому ресурсу.<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | STATUS MESSAGE |<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />| ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33&nbsp; | 10.128.0.6&nbsp; | CLOSING_TRAFFIC [0s] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [33m] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [6h]&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />+----------------------+-------------+----------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; |&nbsp; STATUS&nbsp; |<br />+----------------------+-------------+----------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY&nbsp; |<br />| e2luooifg8ruecr7g6fk | 10.128.0.6&nbsp; | DRAINING |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY&nbsp; |<br />+----------------------+-------------+----------+<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | STATUS MESSAGE |<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />| ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33&nbsp; | 10.128.0.6&nbsp; | CLOSING_TRAFFIC [9s] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [33m] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [6h]&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />+----------------------+-------------+----------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; |&nbsp; STATUS&nbsp; |<br />+----------------------+-------------+----------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY&nbsp; |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY&nbsp; |<br />+----------------------+-------------+----------+ <br />После этого Instance Group начнет пересоздавать удалённую виртуальную машину. Процесс восстановления может занять некоторое время. Понаблюдаем за ним.<br />Сначала новая виртуальная машина появится в группе в статусе CREATING_INSTANCE.<br />+----------------------+---------------------------+----------------+-------------+-------------------------+----------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | STATUS MESSAGE |<br />+----------------------+---------------------------+----------------+-------------+-------------------------+----------------+<br />| ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33&nbsp; | 10.128.0.6&nbsp; | CREATING_INSTANCE [-1s] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [33m]&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [6h]&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+-------------------------+----------------+<br />+----------------------+-------------+----------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; |&nbsp; STATUS&nbsp; |<br />+----------------------+-------------+----------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY&nbsp; |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY&nbsp; |<br />+----------------------+-------------+----------+<br />Далее виртуальная машина будет открыта для трафика (статус OPEN_TRAFFIC). Балансировщик начнет процесс включения машины в список доступных машин.<br />+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS MESSAGE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+<br />| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [-1s] | Adding target(s)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 10.128.0.32 to target group&nbsp;&nbsp;&nbsp; |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | b7rh0bhm9f82dglb2p9r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [34m]&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [7h]&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+<br />+----------------------+-------------+---------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; | STATUS&nbsp; |<br />+----------------------+-------------+---------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY |<br />+----------------------+-------------+---------+<br />+----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS MESSAGE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+<br />| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [1s] | Adding target(s)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 10.128.0.32 to target group&nbsp;&nbsp;&nbsp; |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | b7rh0bhm9f82dglb2p9r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [34m] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [7h]&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+<br />+----------------------+-------------+---------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; | STATUS&nbsp; |<br />+----------------------+-------------+---------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |<br />| e2luooifg8ruecr7g6fk | 10.128.0.32 | INITIAL |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY |<br />+----------------------+-------------+---------+<br />+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS MESSAGE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+<br />| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [18s] | Awaiting HEALTHY state for&nbsp;&nbsp;&nbsp;&nbsp; |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | target(s) 10.128.0.32. Elapsed |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | time: 3s.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [34m]&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [7h]&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+<br />+----------------------+-------------+----------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; |&nbsp; STATUS&nbsp; |<br />+----------------------+-------------+----------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.32 | INITIAL&nbsp; |<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | INACTIVE |<br />| b0c4h992tbuodl5hudpu | 10.128.0.9&nbsp; | HEALTHY&nbsp; |<br />+----------------------+-------------+----------+<br />+----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS MESSAGE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+<br />| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [1m32s] | [NLB unhealthy]; Awaiting&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | HEALTHY state for target(s)&nbsp;&nbsp;&nbsp; |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 10.128.0.32. Elapsed time: 1m&nbsp; |<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 17s.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [35m]&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [7h]&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+<br />+----------------------+-------------+---------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; | STATUS&nbsp; |<br />+----------------------+-------------+---------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |<br />| e2luooifg8ruecr7g6fk | 10.128.0.32 | HEALTHY |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY |<br />+----------------------+-------------+---------+ <br />И в завершение подсеть перейдет в статус HEALTHY, а машина &mdash; в статус RUNNING_ACTUAL, и трафик будет снова разделен между тремя машинами.<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />|&nbsp;&nbsp;&nbsp;&nbsp; INSTANCE ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; EXTERNAL IP&nbsp;&nbsp; | INTERNAL IP |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | STATUS MESSAGE |<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | RUNNING_ACTUAL [-1s] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9&nbsp; | RUNNING_ACTUAL [35m] |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4&nbsp;&nbsp; | 10.128.0.37 | RUNNING_ACTUAL [7h]&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+----------------------+---------------------------+----------------+-------------+----------------------+----------------+<br />+----------------------+-------------+---------+<br />|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SUBNET ID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; ADDRESS&nbsp;&nbsp; | STATUS&nbsp; |<br />+----------------------+-------------+---------+<br />| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |<br />| e2luooifg8ruecr7g6fk | 10.128.0.32 | HEALTHY |<br />| e9bn57jvjnbujnmk3mba | 10.128.0.9&nbsp; | HEALTHY |<br />+----------------------+-------------+---------+ <br />Восстановление произошло автоматически без ручного вмешательства.<br /><strong>Decision:</strong><br />$ vim specification1.yaml<br />$ cat specification1.yaml<br />name: my-group<br />service_account_id: YOUR-ID<br />instance_template:<br />&nbsp;&nbsp;&nbsp; platform_id: standard-v1<br />&nbsp;&nbsp;&nbsp; resources_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory: 2g<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cores: 2<br />&nbsp;&nbsp;&nbsp; boot_disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mode: READ_WRITE<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_id: YOUR-ID1<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; type_id: network-hdd<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 32g<br />&nbsp;&nbsp;&nbsp; network_interface_specs:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - network_id: YOUR-ID2<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subnet_ids: <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - YOUR-ID3<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - YOUR-ID4<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - YOUR-ID5<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}<br />&nbsp;&nbsp;&nbsp; scheduling_policy:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; preemptible: false<br />&nbsp;&nbsp;&nbsp; metadata:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; user-data: |-<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #cloud-config<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; users:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - name: YOUR-USERNAME<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; groups: sudo<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lock_passwd: true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sudo: 'ALL=(ALL) NOPASSWD:ALL'<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ssh-authorized-keys:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - YOUR-KEY<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; package_update: true<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; runcmd:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [ apt-get, install, -y, nginx ]<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']<br />deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 1<br />&nbsp;&nbsp;&nbsp; max_expansion: 0<br />scale_policy:<br />&nbsp;&nbsp;&nbsp; fixed_scale:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size: 3<br />allocation_policy:<br />&nbsp;&nbsp;&nbsp; zones:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-a<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-b<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - zone_id: ru-central1-c<br />load_balancer_spec:<br />&nbsp;&nbsp;&nbsp; target_group_spec:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: my-target-group<br />$ yc compute instance-group create --file specification.yaml<br />$ yc load-balancer network-load-balancer create \<br />&nbsp; --region-id ru-central1 \<br />&nbsp; --name my-load-balancer \<br />&nbsp; --listener name=my-listener,external-ip-version=ipv4,port=80 \<br />&nbsp; --target-group target-group-id=YOUR-ID6,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80<br />$ while true; do \<br />yc compute instance-group \<br />&nbsp; --id YOUR-ID8 list-instances; \<br />yc load-balancer network-load-balancer \<br />&nbsp; --id YOUR-ID7 target-states \<br />&nbsp; --target-group-id YOUR-ID6; \<br />sleep 5; done<br /><strong>Task:</strong><br />Сбой зоны доступности.<br />В этом сценарии рассмотрим ситуацию, когда произошел сбой сразу всей зоны доступности. Такие ситуации возникают крайне редко и могут быть связаны с какими-то масштабными стихийными бедствиями, однако и их стоит предусмотреть.<br />Посмотрим, как будет решаться проблема неожиданного выхода из строя зоны доступности.<br /><strong>Decision:</strong><br />В нашем примере (см. предыдущий урок) используются все три зоны доступности &mdash; ru-central1-a, ru-central1-b и ru-central1-c. В каждой зоне располагается одна ВМ.<br />Установите такие настройки политики развертывания &mdash; пусть группу можно расширять на 1 ВМ и уменьшать на 1 ВМ:<br />Теперь в настройках группы виртуальных машин уберите одну зону доступности, например ru-central1-c. Переключитесь на вкладку Список ВМ и посмотрите, что будет происходить.<br />Для ВМ, которая располагалась в зоне ru-central1-c, отключается трафик (статус Closing traffic), а затем сама машина удаляется (статус Deleting instance). Одновременно в другой зоне доступности создаётся и запускается новая ВМ. Остальные машины в группе продолжают работать без изменений.<br />Таким образом, даже при выходе из строя всей зоны доступности группа виртуальных машин продолжит работать и будет способна принимать прежнюю нагрузку.<br /><strong>Task:</strong><br />Обновление приложения.<br />На практической работе с CLI мы уже рассматривали обновление операционной системы для группы виртуальных машин. Любые приложения, установленные на ВМ, обновляются по тем же правилам. Давайте рассмотрим этот процесс ещё раз.<br /><strong>Decision:</strong><br />Первый вариант обновления<br />Если вы работаете в консоли управления, измените шаблон ВМ и выберите образ с ОС Ubuntu 20.04. Убедитесь, что параметры политики развёртывания такие: группу нельзя расширять, а уменьшать можно только на одну ВМ.<br />Политика развёртывания группы виртуальных машин (вариант 1)<br />Если вы работаете в командной строке, в спецификации specification.yaml измените параметр image_id (например с fd8s2gbn4d5k2rcf12d9 на fd8ju9iqf6g5bcq77jns) и запустите обновление группы:<br />yc compute instance-group update \<br />&nbsp; --name my-group \<br />&nbsp; --file &lt;путь_к_файлу_specification.yaml&gt; <br />В консоли управления на странице группы ВМ перейдите на вкладку Список ВМ и проследите, как меняются статусы машин.<br />Сначала вы увидите статус Running outdated. Это означает, что машины работают со старой версией приложения.<br />Затем одна из машин начинает обновляться: для неё закрывается трафик (статус Closing traffic), она останавливается (статус Stopping instance), обновляется (статус Updating instance), затем трафик снова открывается (статус Opening traffic), и наконец статус меняется на Running actual. Обновление выполнено.<br />Затем то же самое последовательно выполняется для остальных машин в группе.<br />Порядок обновления зависит от политики развёртывания. Мы запретили увеличивать размер группы и указали, что одновременно неработоспособной может быть только одна машина. Именно так и произошло обновление: машины по одной выводились из строя, обновлялись и запускались снова.<br />Второй вариант обновления<br />Теперь давайте изменим настройки политики развёртывания.<br />Если вы работаете в консоли управления, измените шаблон ВМ и выберите образ с Ubuntu и NGINX, созданный ранее и помещённый в Container Registry.<br />Измените параметры развёртывания. Теперь группу можно расширять на одну ВМ, а уменьшать нельзя:<br />Политика развёртывания группы виртуальных машин (вариант 2)<br />Если вы работаете в командной строке, в спецификации specification.yaml измените параметр image_id (например, снова с fd8ju9iqf6g5bcq77jns на fd8s2gbn4d5k2rcf12d9). Параметры обновления измените так:<br />deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 0<br />&nbsp;&nbsp;&nbsp; max_expansion: 1 <br />Запустите обновление группы.<br />В консоли управления на странице группы ВМ перейдите на вкладку Список ВМ и проследите, как меняются статусы машин.<br />Сначала вы увидите статус Running outdated. Затем создаётся новая машина (статус Creating instance), для неё открывается трафик (статус Opening traffic), статус машины меняется на Running actual, при этом одна из &laquo;устаревших&raquo; ВМ выводится из строя (статусы Closing traffic и Stopping instance).<br />Затем то же самое последовательно выполняется для остальных машин в группе.<br /><strong>Decision:</strong><br />$ yc compute instance-group update \<br />&nbsp; --id YOUR-ID \<br />&nbsp; --file specification1.yaml<br />$ cat specification1.yaml<br />...<br />deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 1<br />&nbsp;&nbsp;&nbsp; max_expansion: 0<br />...<br />$ vim specification1.yaml<br />$ cat specification1.yaml<br />...<br />deploy_policy:<br />&nbsp;&nbsp;&nbsp; max_unavailable: 0<br />&nbsp;&nbsp;&nbsp; max_expansion: 1<br />...<br /><strong>Task:</strong><br />Сбой приложения.<br />Последний сценарий, который мы рассмотрим, это сбой приложения. Ситуация, когда сама ВМ работоспособна, но по каким-то причинам произошла ошибка в приложении. Это может быть потеря соединения с базой данных или какой-то баг в запущенном приложении (например утечка памяти). Давайте сымитируем такой сценарий. На наших виртуальных машинах запущен только веб-сервер NGINX, давайте остановим его. Но сначала включим проверку состояния ВМ.<br /><strong>Decision:</strong><br />В консоли управления откройте вкладку Обзор для вашей группы виртуальных машин, нажмите кнопку Изменить и активируйте проверку состояний. Сохраните изменения.<br />В браузере откройте страницу с внешним IP-адресом балансировщика, привязанного к вашей группе, и посмотрите, на какую из машин выводится трафик. Узнайте внешний IP-адрес этой машины.<br />В новой вкладке браузера откройте IP-адрес этой виртуальной машины и убедитесь, что выводится приветственная страница, т. е. сервер доступен.<br />Помните, когда вы меняли файл конфигурации для группы машин, вы добавили в него пользователя my-user? Теперь он вам пригодится &mdash; из консоли зайдите на ВМ от его имени:<br />&nbsp;ssh my-user@&lt;внешний_IP-адрес_ВМ&gt;<br />Посмотрите список запущенных процессов:<br />&nbsp;ps axu<br />Убедитесь, что в списке есть процессы nginx:<br />Теперь остановите эти процессы, чтобы сделать сервер недоступным:<br />&nbsp;sudo killall nginx<br />В браузере обновите страницу балансировщика. Вы увидите, что теперь трафик направляется на другую виртуальную машину группы. Это означает, что Instance Group обнаружил сбой приложения и переключил трафик.<br />Теперь обновите страницу виртуальной машины, на которой вы остановили NGINX. Убедитесь, что сервер теперь недоступен.<br />Откройте список машин вашей группы и проследите, как меняется состояние одной из машин.<br />Сначала будет закрыт трафик (статус Closing traffic), затем виртуальная машина будет остановлена (статус Stopping instance), а затем перезапущена (статус Running actual).<br />Убедитесь, что веб-сервер на этой ВМ снова доступен.<br />Мы проверили четыре основных сценария сбоев и убедились, что Yandex Cloud автоматически отрабатывает их и восстанавливает работоспособность группы.<br />$ ssh YOUR-USERNAME@YOUR-IP<br />$ ps axu<br />$ sudo killall nginx<br /><strong>Task:</strong><br />Отправка собственных метрик.<br />Часто бывает полезно отслеживать более широкий набор метрик, чем тот, что доступен в Yandex Monitoring &laquo;из коробки&raquo;.<br />Предположим, вам интересно узнать, сколько людей заходит на ваш сайт и как их число зависит от времени дня или дня недели. <br />Вы можете выгружать эти данные из Яндекс Метрики или вашей собственной аналитической системы и самостоятельно загружать в Yandex Monitoring с помощью API.<br />Давайте попробуем сделать это с нашим сайтом.<br /><strong>Decision:</strong><br />Отправка метрик через API<br />Получите IAM-токен: Инструкция для аккаунта на Яндексе, Инструкция для сервисного аккаунта.<br />Обратите внимание &mdash; токены устаревают через 12 часов после создания. Поэтому если вы сделаете паузу при выполнении данной практической работы, для продолжения лучше запросить новый токен.<br />Сохраните токен в переменной окружения, так его будет проще использовать:<br />&nbsp;export IAM_TOKEN=&lt;IAM-токен&gt;<br />Создайте файл с телом запроса, например my-metrics.json. В свойстве metrics указывается список метрик для записи. Пусть это будет количество пользователей сайта. В массиве timeseries указываются значения на разные моменты времени (измените число на сегодняшнее в формате год-месяц-день).<br />{<br />&nbsp; "metrics": [<br />&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name": "number_of_users",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "labels": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "site": "aibolit"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "type": "IGAUGE",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "timeseries": [<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T10:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "22"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T11:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "44"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T12:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "11"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T13:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "55"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T14:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "33"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; ]<br />} <br />Отправьте запрос, указав в нем идентификатор каталога и имя сервиса custom (это имя указывается для всех пользовательских метрик):<br />&nbsp;curl -X POST \<br />&nbsp;&nbsp;&nbsp;&nbsp; -H "Content-Type: application/json" \<br />&nbsp;&nbsp;&nbsp;&nbsp; -H "Authorization: Bearer ${IAM_TOKEN}" \<br />&nbsp;&nbsp;&nbsp;&nbsp; -d '@&lt;путь_к_файлу_my-metrics.json&gt;' \<br />&nbsp;'https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write?folderId=&lt;идентификатор_каталога&gt;&amp;service=custom'<br />Мониторинг пользовательских метрик<br />Создайте на вашем дашборде новый виджет с графиком, назовите его &laquo;Число пользователей сайта&raquo;.<br />В виджете создайте запрос с параметрами service = Custom Metrics и name = number_of_users. Убедитесь, что в виджете выбран нужный период:<br />Этот график станет нагляднее, если вместо точек отображать столбцы. Тип графика можно изменить с помощью кнопки в правом верхнем углу виджета:<br />Мониторинг метрик Linux<br />Другой пример &mdash; ваши приложения запущены на виртуальных машинах под Linux. По умолчанию вы можете посмотреть утилизацию ресурсов процессора или диска для ВМ в целом. Но вам будет полезно знать, сколько ресурсов потребляет каждое из них. В Yandex Monitoring вы можете отслеживать системные метрики Linux, такие как объём свободной памяти или загрузка процессора. Но для этого нужно дополнительно настроить отправку этих метрик с помощью Yandex Unified Agent, который мы уже упоминали.<br />Установка Yandex Unified Agent<br />Создайте виртуальную машину. На неё вы будете устанавливать Yandex Unified Agent. Можете использовать образ с ОС Ubuntu, который вы создали ранее и поместили в Container Registry. Назовите машину, например, for-ua.<br />При создании используйте ваш сервисный аккаунт. Задайте логин (например ua-user) и ssh-ключ.<br />Для сервисного аккаунта добавьте роль monitoring.editor.<br />Посмотрите публичный IP-адрес машины for-ua и зайдите на неё по ssh:<br />&nbsp;ssh ua-user@&lt;публичный_адрес_ВМ&gt;<br />Теперь вы можете установить Yandex Unified Agent:<br />&nbsp;ua_version=$(curl -s https://storage.yandexcloud.net/yc-unified-agent/latest-version) bash -c 'curl -s -O https://storage.yandexcloud.net/yc-unified-agent/releases/$ua_version/unified_agent &amp;&amp; chmod +x ./unified_agent'<br />Также вы можете выбрать опцию Установить в поле Агент сбора метрик при создании ВМ, тогда Yandex Unified Agent будет установлен автоматически.<br />Создайте файл config.yml с типовой спецификацией для доставки метрик Linux.<br />В параметре folder_id укажите идентификатор вашего каталога.<br />status:<br />port: "16241"<br />storages:<br />- name: main<br />&nbsp; plugin: fs<br />&nbsp; config:<br />&nbsp;&nbsp;&nbsp; directory: /var/lib/yandex/unified_agent/main<br />&nbsp;&nbsp;&nbsp; max_partition_size: 100mb<br />&nbsp;&nbsp;&nbsp; max_segment_size: 10mb<br />channels:<br />- name: cloud_monitoring<br />&nbsp; channel:<br />&nbsp;&nbsp;&nbsp; pipe:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - storage_ref:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: main<br />&nbsp;&nbsp;&nbsp; output:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; plugin: yc_metrics<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; folder_id: "&lt;идентификатор_каталога&gt;"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iam:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cloud_meta: {}<br />routes:<br />- input:<br />&nbsp;&nbsp;&nbsp; plugin: linux_metrics<br />&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; namespace: sys<br />&nbsp; channel:<br />&nbsp;&nbsp;&nbsp; channel_ref:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: cloud_monitoring<br />- input:<br />&nbsp;&nbsp;&nbsp; plugin: agent_metrics<br />&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; namespace: ua<br />&nbsp; channel:<br />&nbsp;&nbsp;&nbsp; pipe:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - filter:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; plugin: filter_metrics<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; match: "{scope=health}"<br />&nbsp;&nbsp;&nbsp; channel_ref:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: cloud_monitoring<br />import:<br />- /etc/yandex/unified_agent/conf.d/*.yml <br />В секции status достаточно указать порт для просмотра статуса Yandex Unified Agent.<br />Секция storage содержит список хранилищ, в которых будут находиться выгруженные данные. Для практической работы достаточно одного файлового хранилища (fs).<br />Секция channels содержит список именованных каналов, к этим каналам можно обращаться по имени из других секций спецификации. Здесь обозначен один канал с именем cloud_monitoring. К нему идёт обращение из секции routes, которая содержит список маршрутов доставки метрик.<br />Подробнее о конфигурировании Yandex Unified Agent вы можете почитать в документации.<br />Скопируйте файл спецификации в виртуальную машину for-ua:<br />&nbsp;scp config.yml ua-user@84.252.135.237:config.yml<br />Теперь запустите Unified Agent с созданной спецификацией:<br />&nbsp;sudo ./unified_agent --config config.yml<br />Если запуск прошел успешно, в конце вы увидите сообщение такого вида:<br />&nbsp;... NOTICE agent started<br />Настройка виджета для мониторинга метрик Linux. Создайте на вашем дашборде новый виджет с графиком, назовите его &laquo;Метрики Linux&raquo;.<br />В виджете создайте запрос с параметром service = Custom Metrics. В параметре name выберите любой параметр, начинающийся с sys &mdash; всё это системные метрики, поставляемые Unified Agent. Например, name = sys.memory.MemAvailable.<br />Теперь в виджете отображается график наличия свободной оперативной памяти в виртуальной машине for-ua.<br /><strong>Decision:</strong><br />$ yc iam key create --service-account-name monitortest --output key.json<br />$ yc config profile create monitortest-profile<br />$ yc config set service-account-key key.json<br />$ yc iam create-token<br />$ cat key.json<br />...<br />t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA<br />$ export IAM_TOKEN=t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA<br />$ vim my-metrics.json<br />$ cat my-metrics.json<br />{<br />&nbsp; "metrics": [<br />&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name": "number_of_users",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "labels": {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "site": "aibolit"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "type": "IGAUGE",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "timeseries": [<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T10:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "22"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T11:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "44"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T12:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "11"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T13:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "55"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ts": "2021-05-10T14:00:00Z",<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "value": "33"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br />&nbsp;&nbsp;&nbsp; }<br />&nbsp; ]<br />}<br />$ curl -X POST \<br />&nbsp;&nbsp;&nbsp;&nbsp; -H "Content-Type: application/json" \<br />&nbsp;&nbsp;&nbsp;&nbsp; -H "Authorization: Bearer ${t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA}" \<br />&nbsp;&nbsp;&nbsp;&nbsp; -d '@/home/YOUR-DIR/my-metrics.json' \<br />&nbsp;'https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write?folderId=ajehq9p412df22arccm1&amp;service=custom'<br />$ ssh ua-user@&lt;публичный_адрес_ВМ&gt;<br />$ ua_version=$(curl -s https://storage.yandexcloud.net/yc-unified-agent/latest-version) bash -c 'curl -s -O https://storage.yandexcloud.net/yc-unified-agent/releases/$ua_version/unified_agent &amp;&amp; chmod +x ./unified_agent'<br />$ exit<br />$ vim config.yml<br />$ cat config.yml<br />status:<br />&nbsp;port: "16241"<br />storages:<br />&nbsp;- name: main<br />&nbsp;&nbsp;&nbsp;&nbsp; plugin: fs<br />&nbsp;&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; directory: /var/lib/yandex/unified_agent/main<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; max_partition_size: 100mb<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; max_segment_size: 10mb<br />channels:<br />&nbsp;- name: cloud_monitoring<br />&nbsp;&nbsp;&nbsp;&nbsp; channel:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pipe:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - storage_ref:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: main<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; output:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; plugin: yc_metrics<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; folder_id: "&lt;идентификатор_каталога&gt;"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iam:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cloud_meta: {}<br />routes:<br />&nbsp;- input:<br />&nbsp;&nbsp;&nbsp;&nbsp; plugin: linux_metrics<br />&nbsp;&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; namespace: sys<br />&nbsp; channel:<br />&nbsp;&nbsp;&nbsp; channel_ref:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: cloud_monitoring<br />&nbsp;- input:<br />&nbsp;&nbsp;&nbsp;&nbsp; plugin: agent_metrics<br />&nbsp;&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; namespace: ua<br />&nbsp;&nbsp; channel:<br />&nbsp;&nbsp;&nbsp;&nbsp; pipe:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - filter:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; plugin: filter_metrics<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; config:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; match: "{scope=health}"<br />&nbsp;&nbsp;&nbsp;&nbsp; channel_ref:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: cloud_monitoring<br />import:<br />&nbsp;- /etc/yandex/unified_agent/conf.d/*.yml<br />$ scp config.yml ua-user@84.252.135.237:config.yml <br />$ sudo ./unified_agent --config config.yml<br /><strong>Task:</strong><br />Выгрузка метрик в формате Prometheus<br />Как мы уже говорили раньше, метрики можно выгружать из Yandex Cloud Monitoring в сторонние приложения и сервисы. <br />Пожалуй, чаще всего их выгружают для сервера Prometheus.<br />На сегодняшний день Prometheus &mdash; один из самых популярных инструментов для мониторинга приложений и сервисов. В основе его лежит специализированная СУБД для анализа временных рядов, которая обеспечивает высокое быстродействие. В отличие от большинства систем мониторинга, Prometheus не ждёт, пока сторонние приложения передадут ему свои метрики, а сам опрашивает подключенные к нему приложения и собирает нужные данные.<br />Prometheus и Yandex Cloud Monitoring решают схожие задачи &mdash; хранят значения разных метрик. Prometheus фактически является стандартом для обмена метриками. Поэтому даже используя сервисы Yandex Cloud, IT-администраторы часто хотят отслеживать их работу с помощью Prometheus. Чтобы не лишать специалистов привычных инструментов, Yandex Cloud Monitoring поддерживает выгрузку данных в формате Prometheus. Для этого используется метод prometheusMetrics.<br />Для визуализации данных, собираемых Prometheus, можно использовать сервис Grafana (в нем можно зарегистрироваться бесплатно на тестовый период). <br />Вы можете установить Grafana на свой компьютер, а можете работать в облачной версии.<br />Посмотрим, как происходит выгрузка метрик в Prometheus и работа с ними в Grafana. Вы снова будете мониторить сайт клиники &laquo;Доктор Айболит&raquo;.<br /><strong>Decision:</strong><br />Создайте API-ключ через консоль управления Yandex Cloud или CLI.<br />Если вы создаете ключ в консоли управления, то перейдите в каталог, из которого будете выгружать метрики (например default). Затем перейдите на вкладку Сервисные аккаунты и выберите существующий аккаунт. Нажмите кнопку Создать новый ключ и выберите Создать API-ключ. В описании ключа можно указать, например, &laquo;для доступа к Prometheus&raquo;. Сохраните секретную часть ключа в отдельный файл, например, prometheus-key.txt.<br />Назначьте сервисному аккаунту роль monitoring.viewer на выбранный каталог.<br />Создайте файл спецификации prometheus.yml (см. пример ниже, замените в нем значение параметра folderId на идентификатор каталога, а значение для bearer_token &mdash; на ключ доступа из файла prometheus-key.txt):<br />global:<br />&nbsp; scrape_interval:&nbsp;&nbsp;&nbsp;&nbsp; 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.<br />&nbsp; evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.<br />&nbsp; # scrape_timeout is set to the global default (10s).<br />rule_files:<br />scrape_configs:<br />&nbsp; - job_name: 'prometheus'<br />&nbsp;&nbsp;&nbsp; static_configs:<br />&nbsp;&nbsp;&nbsp; - targets: ['localhost:9090'] <br />&nbsp; - job_name: 'yc-monitoring-export'<br />&nbsp;&nbsp;&nbsp; metrics_path: '/monitoring/v2/prometheusMetrics'<br />&nbsp;&nbsp;&nbsp; params:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; folderId:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - '&lt;идентификатор_каталога&gt;' <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; service:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 'storage' <br />&nbsp;&nbsp;&nbsp; bearer_token: '&lt;секретная_часть_API-ключа&gt;'<br />&nbsp;&nbsp;&nbsp; static_configs:<br />&nbsp;&nbsp;&nbsp; - targets: ['monitoring.api.cloud.yandex.net']<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; labels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; folderId: '&lt;идентификатор_каталога&gt;'<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; service: 'storage' <br />Запуск сервера Prometheus. Если вы уже работаете с Prometheus, пропустите все шаги по установке &mdash; просто добавьте секцию scrape_configs из примера выше в спецификацию вашего сервера Prometheus и перезапустите сервер, а затем переходите к настройке Grafana.<br />Для запуска сервера Prometheus используйте официальный Docker-образ prom/prometheus.<br />Сначала загрузите образ. Для этого запустите Docker Desktop (в терминале выполните команду):<br />&nbsp;docker pull prom/prometheus<br />Чтобы на сервере сразу был ваш файл спецификации, создайте свой образ на основе prom/prometheus. Подготовьте Dockerfile с двумя командами:<br />&nbsp;FROM prom/prometheus<br />&nbsp;ADD prometheus.yml /etc/prometheus/<br />Сохраните этот файл в тот же каталог, где находится prometheus.yml. Назовите его именем по умолчанию: Dockerfile.<br />В терминале перейдите в каталог с Dockerfile. Создайте образ с вашей конфигурацией (используйте ваш идентификатор в Yandex Container Registry):<br />&nbsp;docker build . -t cr.yandex/&lt;идентификатор_реестра&gt;/my-prometheus:latest -f Dockerfile<br />Аутентифицируйтесь в Yandex Container Registry с помощью Docker Credential helper (чтобы Docker мог от вашего имени отправить образ в ваш реестр):<br />&nbsp;yc container registry configure-docker<br />Теперь отправьте образ в ваше хранилище в облаке:<br />&nbsp;docker push cr.yandex/&lt;идентификатор_реестра&gt;/my-prometheus:latest<br />Создайте виртуальную машину с помощью Container Optimized Image, вы уже делали это раньше в практической работе (в разделе Выбор образа загрузочного диска переключитесь на вкладку Container Solution и нажмите Настроить. Выберите из реестра созданный вами образ, остальные настройки оставьте по умолчанию и нажмите Применить).<br />При создании виртуальной машины используйте ваш сервисный аккаунт. Задайте логин (например prom) и ssh-ключ.<br />Назовите машину, например, for-prometheus.<br />Проверьте статус сервера по адресу http://&lt;публичный IP-адрес ВМ с Prometheus&gt;:9090/targets. Через несколько минут после запуска статус процессов prometheus и yc-monitoring-export должен стать UP.<br />Подайте нагрузку на ваш сайт:<br />while true; do wget -q -O- &lt;адрес_сайта&gt;; done <br />Подождите несколько минут и проверьте, как поставляются метрики в Prometheus.<br />В верхнем меню выберите пункт Graph. Нажмите на значок &laquo;Земли&raquo;. Откроется меню с доступными метриками. Выберите метрику, которую вы хотите проверить, например, traffic и нажмите кнопку Execute.<br />Переключитесь на вкладку Graph. Выберите текущее время, для наглядности уменьшите интервал запроса данных (например до 15 минут). Вскоре вы увидите график изменения выбранной метрики.<br />Настройка Grafana. Теперь посмотрим, как метрики визуализируются в системе Grafana.<br />Если у вас еще нет аккаунта в Grafana, создайте его с помощью нескольких простых шагов, это бесплатно. Вам откроется интерфейс по адресу https://&lt;ваш_логин&gt;.grafana.net/.<br />Добавление источника данных. Настройте Prometheus в качестве источника данных. На главной странице нажмите кнопку Connect data. Из предложенного списка выберите источник Prometheus data source и нажмите кнопку Create Prometheus data source.<br />В следующем окне в поле URL введите endpoint сервера Prometheus http://&lt;публичный IP-адрес ВМ с Prometheus&gt;:9090. Больше никакие настройки менять не нужно.<br />Внизу нажмите кнопку Save &amp; Test. Должна отобразиться надпись Data source is working.<br />Добавление дашборда. Вернитесь на главную страницу (нажав на логотип в левом верхнем углу) и нажмите Create your first dashboard. Откроется окно настройки дашборда.<br />В нижней части экрана на вкладке Query выберите источник данных &mdash; Prometheus.<br />Выберите метрику, которую вы хотите отслеживать. Нажмите на поле Metrics, в открывшемся списке выберите метрику traffic.<br />Сверху отобразится график выбранной метрики.<br />Вверху справа в поле Panel Title укажите название графика (например, &laquo;Трафик сайта&raquo;).<br />Теперь сохраните настройки &mdash; в правом верхнем углу нажмите кнопку Save и укажите название дашборда (например, &laquo;Мой дашборд&raquo;).<br /><strong>Decision:</strong><br />$ vim prometheus-key.txt<br />$ cat prometheus-key.txt<br />YOUR-KEY<br />$ vim prometheus.yml<br />$ cat prometheus.yml<br />global:<br />&nbsp; scrape_interval:&nbsp;&nbsp;&nbsp;&nbsp; 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.<br />&nbsp; evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.<br />&nbsp; # scrape_timeout is set to the global default (10s).<br />rule_files:<br />scrape_configs:<br />&nbsp; - job_name: 'prometheus'<br />&nbsp;&nbsp;&nbsp; static_configs:<br />&nbsp;&nbsp;&nbsp; - targets: ['localhost:9090']<br />&nbsp; - job_name: 'yc-monitoring-export'<br />&nbsp;&nbsp;&nbsp; metrics_path: '/monitoring/v2/prometheusMetrics'<br />&nbsp;&nbsp;&nbsp; params:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; folderId:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 'YOUR-ID' <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; service:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 'storage' <br />&nbsp;&nbsp;&nbsp; bearer_token: 'YOUR-KEY'<br />&nbsp;&nbsp;&nbsp; static_configs:<br />&nbsp;&nbsp;&nbsp; - targets: ['monitoring.api.cloud.yandex.net']<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; labels:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; folderId: 'YOUR-ID'<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; service: 'storage'<br />$ docker pull prom/prometheus<br />$ vim Dockerfile<br />$ cat Dockerfile<br />FROM prom/prometheus<br />ADD prometheus.yml /etc/prometheus/<br />$ docker build . -t cr.yandex/YOUR-ID1/my-prometheus:latest -f Dockerfile<br />$ yc container registry configure-docker<br />$ docker push cr.yandex/YOUR-ID1/my-prometheus:latest<br />$ while true; do wget -q -O- YOUR-IP; done &nbsp;<br /><strong>Task:</strong><br />В этой практической работе вы создадите алерт для случая, если трафик на сайте вдруг начнет существенно расти. <br />Снова используйте сайт клиники &laquo;Доктор Айболит&raquo;, для которого настраивали графики на дашборде.<br />Вы можете перейти на вкладку Алерты и там настроить алерт с нуля. А можете отталкиваться от графиков, которые уже выведены в виджете. <br />Ниже рассматривается именно второй вариант.<br /><strong>Decision:</strong><br />Создание алерта. Вернитесь на созданный вами дашборд и в меню виджета &laquo;Трафик сайта&raquo; выберите пункт Создать алерт.<br />Поскольку в виджете используются два запроса, вам будет предложено выбрать, для какого запроса вы хотите создать алерт. Выберите запрос с суммирующей функцией и нажмите Продолжить.<br />Теперь задайте имя и, если хотите, описание алерта. Укажите значение для статусов Alarm и Warning.<br />Откройте спойлер Показать дополнительные настройки. Там вы увидите, что система предложила вам использовать среднее значение за 5 минут. Оставьте эти параметры.<br />Теперь нужно выбрать канал для получения алертов. У вас пока ещё нет настроенных каналов, поэтому система предложить вам создать его. Нажмите кнопку Добавить канал и далее Создать канал.<br />Укажите имя канала, выберите метод &mdash; Email, SMS или Push-уведомления. Укажите получателей &mdash; себя. Затем нажмите кнопку Создать.<br />В настройках алерта выберите только что созданный канал.<br />Вы можете указать для одного алерта несколько каналов уведомлений. Например, если вы хотите получать алерты об увеличении трафика сайта не только в виде Push-уведомлений, но и по электронной почте, создайте еще один канал с методом Email и выберите также и его.<br />Для каждого канала можно настроить режим повторения уведомлений. Например, в данном случае при превышении трафика будет отправлен один алерт по электронной почте, а алерты в виде push-уведомлений будут отправляться каждые 5 минут до тех пор, пока проблема не будет устранена.<br />Нажмите кнопку Создать алерт.<br />Вы увидите настройки созданного алерта, а сверху &mdash; его текущий статус OK.<br />Нажмите слева на вкладку Алерты. Вы увидите ваш алерт, сейчас он единственный в списке. Когда алертов станет больше, вам понадобятся инструменты для работы с ними. Например, вы сможете отобрать из списка только алерты, имеющие статус Alarm или Warning, или временно деактивировать отдельные алерты.<br />Срабатывание алерта. Теперь посмотрим, как срабатывает алерт. Подайте трафик на сайт, который вы мониторите:<br />while true; do wget -q -O- &lt;адрес_сайта&gt;; done <br />Подождите немного и понаблюдайте за ростом нагрузки. Через какое-то время трафик начнет превышать пороговое значение Warning, и вы начнете получать Push-уведомления.<br />Если у администратора настроены другие каналы для алертов, он получил бы SMS или Email с предупреждением о пороговом значении трафика.<br />Как видите, алерты позволяют вовремя привлекать внимание администратора и устранять даже потенциальные, ещё не случившиеся проблемы.<br /><strong>Decision:</strong><br />$ while true; do wget -q -O- YOUR-IP; done</p>
